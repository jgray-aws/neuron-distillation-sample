{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Generate Teacher Logits\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will learn how to generate teacher model logits for knowledge distillation using AWS Neuron. Knowledge distillation is a model compression technique where a smaller \"student\" model learns to mimic the behavior of a larger \"teacher\" model by training on the teacher's output probabilities (logits) rather than just the hard labels.\n",
    "\n",
    "You will use the [Qwen3-30B-A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B) model as your teacher model - a 30 billion parameter Mixture-of-Experts (MoE) model that has been optimized for efficient inference. This teacher model will process a sentiment classification dataset and generate logits that capture the model's \"soft\" predictions across all possible output tokens.\n",
    "\n",
    "The generated logits will be saved to a JSON file and used in Lab 1 to train a smaller student model that can achieve similar performance with significantly fewer parameters and lower inference costs.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Teacher Model**: A large, high-performance model (Qwen3-30B-A3B) that generates training signals\n",
    "- **Logits**: Raw prediction scores before softmax, containing richer information than hard labels\n",
    "- **Neuron Compilation**: Converting PyTorch models to run efficiently on AWS Trainium/Inferentia accelerators\n",
    "- **Tensor Parallelism**: Distributing model layers across multiple NeuronCores for large model inference\n",
    "\n",
    "**Prerequisites:**\n",
    "- AWS EC2 instance with Trainium/Inferentia (e.g., trn2.3xlarge)\n",
    "- AWS Neuron SDK installed (see setup.sh in the repository root)\n",
    "- Sufficient disk space for model compilation artifacts\n",
    "- Dataset file at `data/dataset.txt` with one text sample per line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model weights\n",
    "\n",
    "First, download the teacher model weights from HuggingFace, using the HuggingFace CLI. The model detail page can be found here: [Qwen3-30B-A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download Qwen/Qwen3-30B-A3B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "Import the required libraries for running inference with the Qwen3 MoE model on AWS Neuron hardware:\n",
    "\n",
    "- **torch**: PyTorch framework for tensor operations and model execution\n",
    "- **json**: For saving logits and results to structured JSON files\n",
    "- **transformers**: Hugging Face library providing the tokenizer and generation utilities\n",
    "- **neuronx_distributed_inference**: AWS Neuron SDK package for distributed inference on Trainium/Inferentia\n",
    "  - `MoENeuronConfig`: Configuration for Mixture-of-Experts models on Neuron\n",
    "  - `OnDeviceSamplingConfig`: Controls sampling behavior (temperature, top-k, top-p) on the Neuron device\n",
    "  - `Qwen3MoeInferenceConfig`: Qwen3-specific inference configuration\n",
    "  - `NeuronQwen3MoeForCausalLM`: Neuron-optimized Qwen3 model implementation\n",
    "  - `HuggingFaceGenerationAdapter`: Adapter to use Hugging Face generation API with Neuron models\n",
    "\n",
    "The random seed is set for reproducibility of generation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from neuronx_distributed_inference.models.config import MoENeuronConfig, OnDeviceSamplingConfig\n",
    "from neuronx_distributed_inference.models.qwen3_moe.modeling_qwen3_moe import Qwen3MoeInferenceConfig, NeuronQwen3MoeForCausalLM\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the paths and file locations for this lab:\n",
    "\n",
    "- **model_path**: Hugging Face model identifier for the Qwen3-30B-A3B teacher model. This will be downloaded from the Hugging Face Hub on first use.\n",
    "- **traced_model_path**: Local directory where the Neuron-compiled model artifacts will be saved. The compiled model can be reused for subsequent runs, significantly reducing startup time.\n",
    "- **dataset_file**: Path to the input dataset containing text samples (one per line) that will be processed by the teacher model. Each line should contain a single text sample for sentiment classification.\n",
    "- **output_file**: Path where the generated logits and responses will be saved in JSON format\n",
    "\n",
    "**Dataset Examples:**\n",
    "\n",
    "The dataset contains various text samples with different sentiments:\n",
    "- *Positive*: \"The service at this restaurant exceeded all my expectations!\"\n",
    "- *Negative*: \"I can't believe how rude the staff was today.\"\n",
    "- *Neutral*: \"The weather is 72 degrees with partial clouds.\"\n",
    "- *Negative*: \"My flight was delayed for the third time this week.\"\n",
    "- *Neutral*: \"The package arrived on schedule as expected.\"\n",
    "- *Positive*: \"This phone's battery life is absolutely amazing!\"\n",
    "\n",
    "**Note:** The first time you run this notebook, the model will be compiled for Neuron hardware. This is a one-time operation that optimizes the model for efficient execution on Trainium/Inferentia. Subsequent runs will load the pre-compiled model from `traced_model_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Qwen/Qwen3-30B-A3B\"\n",
    "traced_model_path = \"/home/ubuntu/traced_model/Qwen3-30B-A3B/\"\n",
    "dataset_file = \"data/dataset.txt\"\n",
    "output_file = \"output.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Conversation Template\n",
    "\n",
    "Define a function to format input text as a structured conversation for the sentiment classification task.\n",
    "\n",
    "The Qwen3 model expects inputs in a chat format with distinct roles (system, user, assistant). This function:\n",
    "1. Creates a system message that defines the task: classifying sentiment as POSITIVE, NEGATIVE, or NEUTRAL\n",
    "2. Formats the input sample as a user message\n",
    "3. Returns a conversation list that will be processed by the tokenizer's chat template\n",
    "\n",
    "This structured format helps the model understand its role and produce consistent, focused outputs. The system message explicitly instructs the model to return only the sentiment label, which is important for generating clean training data for the student model.\n",
    "\n",
    "**Example Conversation Structure:**\n",
    "\n",
    "For the input text: \"This phone's battery life is absolutely amazing!\"\n",
    "\n",
    "The function creates:\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"This phone's battery life is absolutely amazing!\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Expected teacher model response: \"POSITIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(sample):\n",
    "    system_message = (\n",
    "        \"You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\"\n",
    "    )\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": sample\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Configuration\n",
    "\n",
    "Configure the Neuron-specific settings for optimal inference with the Qwen3 MoE model. This configuration is critical for both performance and functionality:\n",
    "\n",
    "**MoENeuronConfig Parameters:**\n",
    "- **tp_degree=8**: Tensor parallelism degree - distributes the model across 8 NeuronCores. For a 30B parameter model, this parallelism is essential for fitting the model in memory and achieving good throughput. Adjust based on your instance type (e.g., trn2.3xlarge has 8 NeuronCores).\n",
    "- **batch_size=1**: Process one sample at a time. For logit generation, we prioritize accuracy over throughput.\n",
    "- **max_context_length=128**: Maximum input sequence length in tokens. Shorter contexts reduce memory usage and compilation time.\n",
    "- **seq_len=1024**: Maximum total sequence length (input + output). This determines the buffer size for generation.\n",
    "- **on_device_sampling_config**: Controls generation behavior on the Neuron device:\n",
    "  - `do_sample=True`: Enable sampling (vs greedy decoding) for more diverse outputs\n",
    "  - `temperature=0.6`: Lower temperature = more focused predictions (range: 0.0-1.0)\n",
    "  - `top_k=20`: Consider only the top 20 most likely tokens at each step\n",
    "  - `top_p=0.95`: Nucleus sampling - consider tokens with cumulative probability up to 95%\n",
    "- **enable_bucketing=False**: Disable dynamic sequence length bucketing for consistent performance\n",
    "- **flash_decoding_enabled=False**: Disable flash attention optimization (may not be needed for small batches)\n",
    "- **output_scores=True**: Return generation scores for each token\n",
    "- **output_logits=True**: **CRITICAL** - Return raw logits for knowledge distillation. This is the key output we need for training the student model.\n",
    "\n",
    "**Tokenizer Configuration:**\n",
    "- **padding_side=\"right\"**: Add padding tokens to the right side of sequences\n",
    "- **pad_token = eos_token**: Use the end-of-sequence token for padding (Qwen3 doesn't have a dedicated pad token)\n",
    "\n",
    "These settings balance inference quality, memory efficiency, and the specific requirements of knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "\n",
    "neuron_config = MoENeuronConfig(\n",
    "    tp_degree=8,\n",
    "    batch_size=1,\n",
    "    max_context_length=128,\n",
    "    seq_len=1024,\n",
    "    on_device_sampling_config=OnDeviceSamplingConfig(do_sample=True, temperature=0.6, top_k=20, top_p=0.95),\n",
    "    enable_bucketing=False,\n",
    "    flash_decoding_enabled=False,\n",
    "    output_scores=True,\n",
    "    output_logits=True\n",
    ")\n",
    "\n",
    "config = Qwen3MoeInferenceConfig(\n",
    "    neuron_config,\n",
    "    load_config=load_pretrained_config(model_path),\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Save Model\n",
    "\n",
    "Compile the model for AWS Neuron hardware. This is a critical optimization step that converts the PyTorch model into a Neuron-optimized format.\n",
    "\n",
    "**What happens during compilation:**\n",
    "1. The model is loaded from Hugging Face Hub (if not already cached locally)\n",
    "2. The Neuron compiler analyzes the model architecture and applies hardware-specific optimizations\n",
    "3. Model layers are partitioned across the specified number of NeuronCores (tp_degree=8)\n",
    "4. The compiled model artifacts (NEFF files) are saved to `traced_model_path`\n",
    "5. The tokenizer is also saved to the same directory for convenience\n",
    "\n",
    "**Important Notes:**\n",
    "- **First-time compilation takes 30-60 minutes** depending on model size and instance type. This is normal!\n",
    "- Compilation is a one-time cost - subsequent runs will load the pre-compiled model in seconds\n",
    "- The compiled model is specific to the configuration (tp_degree, batch_size, seq_len, etc.). Changing these parameters requires recompilation.\n",
    "- Ensure you have sufficient disk space (~50GB) for compilation artifacts\n",
    "- If compilation fails, check CloudWatch logs for detailed error messages\n",
    "\n",
    "**Tip:** If you already have a compiled model at `traced_model_path`, you can skip this cell and proceed directly to the \"Load Compiled Model\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCompiling and saving model...\")\n",
    "model = NeuronQwen3MoeForCausalLM(model_path, config)\n",
    "model.compile(traced_model_path)\n",
    "tokenizer.save_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Compiled Model\n",
    "\n",
    "Load the pre-compiled Neuron model from disk for fast inference.\n",
    "\n",
    "This step loads the compiled model artifacts (NEFF files) that were saved in the previous step. Loading a compiled model is much faster than compilation - typically taking only a few seconds.\n",
    "\n",
    "The model is loaded onto the NeuronCores and is ready for inference. The tokenizer is also loaded from the same directory to ensure consistency with the model's vocabulary and special tokens.\n",
    "\n",
    "**Note:** If you're running this notebook for the first time, make sure you've completed the \"Compile and Save Model\" step above. If you're restarting the notebook or running on a different instance with the compiled model already available, you can start from this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuronQwen3MoeForCausalLM(traced_model_path)\n",
    "model.load(traced_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset and Generate Logits\n",
    "\n",
    "Process each text sample in the dataset through the teacher model to generate logits for knowledge distillation.\n",
    "\n",
    "**Processing Pipeline:**\n",
    "\n",
    "For each line in the dataset file:\n",
    "\n",
    "1. **Format Input**: Convert the raw text into a structured conversation using the `create_conversation()` function\n",
    "2. **Apply Chat Template**: Use the tokenizer's chat template to format the conversation with proper special tokens and structure\n",
    "3. **Tokenize**: Convert the formatted text into token IDs that the model can process\n",
    "4. **Generate**: Run inference with the Neuron model to generate the sentiment classification\n",
    "   - `return_dict_in_generate=True`: Return detailed generation outputs\n",
    "   - `output_scores=True`: Include per-token scores\n",
    "   - `output_logits=True`: Include raw logits (essential for distillation)\n",
    "5. **Extract Logits**: Process the raw logits for each generated token:\n",
    "   - Filter out `-inf` values (these represent invalid/masked tokens)\n",
    "   - Store both the token indices and their corresponding logit values\n",
    "   - This sparse representation saves significant storage space\n",
    "6. **Decode Output**: Convert generated token IDs back to human-readable text\n",
    "7. **Store Results**: Save the prompt, generated text, and token logits to the results list\n",
    "\n",
    "**Example Processing:**\n",
    "\n",
    "Input: \"Our team just won the championship - best day ever!\"\n",
    "\n",
    "Output structure:\n",
    "```python\n",
    "{\n",
    "  \"prompt\": \"Our team just won the championship - best day ever!\",\n",
    "  \"response\": {\n",
    "    \"generated_text\": \"POSITIVE\",\n",
    "    \"token_logits\": [\n",
    "      {\n",
    "        \"indices\": [12345, 23456, 34567, ...],  # Token IDs with finite logits\n",
    "        \"logits\": [8.2, 7.5, 6.8, ...]           # Corresponding logit values\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Error Handling:**\n",
    "If any sample fails to process (e.g., due to length constraints or generation issues), the error is caught and logged, allowing the pipeline to continue with remaining samples.\n",
    "\n",
    "**Why Filter Infinite Logits?**\n",
    "The model's vocabulary contains ~150K tokens, but most are invalid for any given generation step (e.g., special tokens, tokens that don't make sense in context). The model assigns `-inf` logits to these invalid tokens. By filtering them out, we:\n",
    "- Reduce storage requirements by 95%+\n",
    "- Speed up student model training (fewer values to process)\n",
    "- Maintain all meaningful information (the student only needs to learn from valid token distributions)\n",
    "\n",
    "**Expected Runtime:**\n",
    "Processing time depends on dataset size and generation length. For a dataset with 100 samples, expect ~5-10 minutes of processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "with open(dataset_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                input_text = create_conversation(line.strip())\n",
    "                formatted_chat = tokenizer.apply_chat_template(\n",
    "                    input_text,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                    enable_thinking=False\n",
    "                )\n",
    "                inputs = tokenizer(formatted_chat, padding=True, return_tensors=\"pt\")\n",
    "                generation_model = HuggingFaceGenerationAdapter(model)\n",
    "                outputs = generation_model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    generation_config=generation_config,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_length=model.config.neuron_config.max_length,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    output_logits=True\n",
    "                )\n",
    "                \n",
    "                print(outputs)\n",
    "                generated_tokens = outputs.sequences[0]\n",
    "                token_logits = outputs.scores\n",
    "                generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                print(generated_text)\n",
    "                \n",
    "                token_logits_list = []\n",
    "                for logits in token_logits:\n",
    "                    finite_mask = torch.isfinite(logits[0])\n",
    "                    finite_indices = torch.nonzero(finite_mask).squeeze().tolist()\n",
    "                    finite_logits = logits[0][finite_mask]\n",
    "                    token_info = {\n",
    "                        'indices': finite_indices,\n",
    "                        'logits': finite_logits.tolist()\n",
    "                    }\n",
    "                    token_logits_list.append(token_info)\n",
    "                \n",
    "                print(token_logits_list)\n",
    "                results.append({\n",
    "                    'prompt': line.strip(),\n",
    "                    'response': {\n",
    "                        'generated_text': generated_text,\n",
    "                        'token_logits': token_logits_list\n",
    "                    }\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing prompt: {line[:50]}...\")\n",
    "                print(f\"Error message: {str(e)}\")\n",
    "                results.append({\n",
    "                    'prompt': line.strip(),\n",
    "                    'error': str(e)\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Write the generated logits and responses to a JSON file for use in Lab 1 (distillation training).\n",
    "\n",
    "The output JSON file contains an array of objects, where each object represents one processed sample:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"prompt\": \"The service at this restaurant exceeded all my expectations!\",\n",
    "    \"response\": {\n",
    "      \"generated_text\": \"POSITIVE\",\n",
    "      \"token_logits\": [\n",
    "        {\n",
    "          \"indices\": [12345, 23456, 34567, 45678, 56789],\n",
    "          \"logits\": [8.5, 7.2, 6.1, 5.8, 5.3]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"prompt\": \"I can't believe how rude the staff was today.\",\n",
    "    \"response\": {\n",
    "      \"generated_text\": \"NEGATIVE\",\n",
    "      \"token_logits\": [\n",
    "        {\n",
    "          \"indices\": [98765, 87654, 76543, 65432, 54321],\n",
    "          \"logits\": [9.1, 8.3, 7.5, 6.9, 6.2]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"prompt\": \"The weather is 72 degrees with partial clouds.\",\n",
    "    \"response\": {\n",
    "      \"generated_text\": \"NEUTRAL\",\n",
    "      \"token_logits\": [\n",
    "        {\n",
    "          \"indices\": [11111, 22222, 33333, 44444, 55555],\n",
    "          \"logits\": [7.8, 7.1, 6.5, 5.9, 5.4]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Output Structure:**\n",
    "- **prompt**: The original input text from the dataset\n",
    "- **response.generated_text**: The teacher model's generated sentiment classification (POSITIVE, NEGATIVE, or NEUTRAL)\n",
    "- **response.token_logits**: Array of logit distributions for each generated token\n",
    "  - **indices**: Token IDs that have finite (valid) logit values\n",
    "  - **logits**: The corresponding logit values for those tokens\n",
    "\n",
    "**Next Steps:**\n",
    "This output file will be used in Lab 1 to train a smaller student model. The student model will learn to match the teacher's logit distributions, effectively transferring the teacher's knowledge to a more efficient model.\n",
    "\n",
    "**File Size:**\n",
    "Even with filtering, the output file can be large (several MB to GB depending on dataset size). Ensure you have sufficient disk space and consider the file size when transferring or storing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Processing complete! Processed {len(results)} prompts. Results written to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_7",
   "language": "python",
   "name": "aws_neuronx_venv_pytorch_2_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
