{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Generate Teacher Logits\n",
    "\n",
    "This notebook generates teacher model logits for knowledge distillation. The teacher model (Qwen3-30B-A3B) processes a dataset and outputs logits that will be used to train a smaller student model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "Import required libraries for model inference, tokenization, and Neuron-specific configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from neuronx_distributed_inference.models.config import MoENeuronConfig, OnDeviceSamplingConfig\n",
    "from neuronx_distributed_inference.models.qwen3_moe.modeling_qwen3_moe import Qwen3MoeInferenceConfig, NeuronQwen3MoeForCausalLM\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set model paths and file locations. The teacher model will be compiled and saved to the traced model path for efficient inference on AWS Neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Qwen/Qwen3-30B-A3B\"\n",
    "traced_model_path = \"/home/ubuntu/traced_model/Qwen3-30B-A3B/\"\n",
    "dataset_file = \"data/dataset.txt\"\n",
    "output_file = \"output.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Conversation Template\n",
    "\n",
    "Define a function to format input text as a conversation for the sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(sample):\n",
    "    system_message = (\n",
    "        \"You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\"\n",
    "    )\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": sample\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Configuration\n",
    "\n",
    "Configure the Neuron-specific settings for the Qwen3 MoE model:\n",
    "- Tensor parallelism degree: 8 (distributes model across 8 NeuronCores)\n",
    "- Enable logit output for distillation\n",
    "- Set sampling parameters for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "\n",
    "neuron_config = MoENeuronConfig(\n",
    "    tp_degree=8,\n",
    "    batch_size=1,\n",
    "    max_context_length=128,\n",
    "    seq_len=1024,\n",
    "    on_device_sampling_config=OnDeviceSamplingConfig(do_sample=True, temperature=0.6, top_k=20, top_p=0.95),\n",
    "    enable_bucketing=False,\n",
    "    flash_decoding_enabled=False,\n",
    "    output_scores=True,\n",
    "    output_logits=True\n",
    ")\n",
    "\n",
    "config = Qwen3MoeInferenceConfig(\n",
    "    neuron_config,\n",
    "    load_config=load_pretrained_config(model_path),\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Save Model\n",
    "\n",
    "Compile the model for AWS Neuron hardware. This step converts the model to a Neuron-optimized format and saves it for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCompiling and saving model...\")\n",
    "model = NeuronQwen3MoeForCausalLM(model_path, config)\n",
    "model.compile(traced_model_path)\n",
    "tokenizer.save_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Compiled Model\n",
    "\n",
    "Load the compiled model from disk for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuronQwen3MoeForCausalLM(traced_model_path)\n",
    "model.load(traced_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset and Generate Logits\n",
    "\n",
    "Process each line in the dataset through the teacher model:\n",
    "1. Format input as a conversation\n",
    "2. Generate output with logits\n",
    "3. Extract finite logits (filter out -inf values)\n",
    "4. Save results with prompt, generated text, and token logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "with open(dataset_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                input_text = create_conversation(line.strip())\n",
    "                formatted_chat = tokenizer.apply_chat_template(\n",
    "                    input_text,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                    enable_thinking=False\n",
    "                )\n",
    "                inputs = tokenizer(formatted_chat, padding=True, return_tensors=\"pt\")\n",
    "                generation_model = HuggingFaceGenerationAdapter(model)\n",
    "                outputs = generation_model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    generation_config=generation_config,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_length=model.config.neuron_config.max_length,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    output_logits=True\n",
    "                )\n",
    "                \n",
    "                print(outputs)\n",
    "                generated_tokens = outputs.sequences[0]\n",
    "                token_logits = outputs.scores\n",
    "                generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                print(generated_text)\n",
    "                \n",
    "                token_logits_list = []\n",
    "                for logits in token_logits:\n",
    "                    finite_mask = torch.isfinite(logits[0])\n",
    "                    finite_indices = torch.nonzero(finite_mask).squeeze().tolist()\n",
    "                    finite_logits = logits[0][finite_mask]\n",
    "                    token_info = {\n",
    "                        'indices': finite_indices,\n",
    "                        'logits': finite_logits.tolist()\n",
    "                    }\n",
    "                    token_logits_list.append(token_info)\n",
    "                \n",
    "                print(token_logits_list)\n",
    "                results.append({\n",
    "                    'prompt': line.strip(),\n",
    "                    'response': {\n",
    "                        'generated_text': generated_text,\n",
    "                        'token_logits': token_logits_list\n",
    "                    }\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing prompt: {line[:50]}...\")\n",
    "                print(f\"Error message: {str(e)}\")\n",
    "                results.append({\n",
    "                    'prompt': line.strip(),\n",
    "                    'error': str(e)\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Write the generated logits and responses to a JSON file for use in distillation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Processing complete! Processed {len(results)} prompts. Results written to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_7",
   "language": "python",
   "name": "aws_neuronx_venv_pytorch_2_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
