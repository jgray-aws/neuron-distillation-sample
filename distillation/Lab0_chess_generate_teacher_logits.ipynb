{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Generate Teacher Logits for Chess Move Evaluation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will generate teacher model logits for chess move evaluation using the MATE_DATASET. The task is to evaluate two candidate chess moves and determine which one is better.\n",
    "\n",
    "**Dataset**: [OutFlankShu/MATE_DATASET](https://huggingface.co/datasets/OutFlankShu/MATE_DATASET)\n",
    "\n",
    "**Task Format**:\n",
    "- **Instruction**: System prompt explaining the chess evaluation task\n",
    "- **Input**: FEN board position + two candidate moves with strategies and tactics\n",
    "- **Output**: The better move (e.g., \"MoveA:d2d8\" or \"MoveB:d4e6\")\n",
    "\n",
    "**Teacher Model**: Qwen3-30B-A3B (30B parameter MoE model)\n",
    "\n",
    "**Prerequisites**:\n",
    "- AWS EC2 instance with Trainium/Inferentia (e.g., trn1.32xlarge or inf2.48xlarge)\n",
    "- AWS Neuron SDK installed\n",
    "- Virtual environment activated: `source /opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/bin/activate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model Weights\n",
    "\n",
    "Download the Qwen3-30B-A3B teacher model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download Qwen/Qwen3-30B-A3B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from neuronx_distributed_inference.models.config import MoENeuronConfig, OnDeviceSamplingConfig\n",
    "from neuronx_distributed_inference.models.qwen3_moe.modeling_qwen3_moe import Qwen3MoeInferenceConfig, NeuronQwen3MoeForCausalLM\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Qwen/Qwen3-30B-A3B\"\n",
    "traced_model_path = \"/home/ubuntu/traced_model/Qwen3-30B-A3B/\"\n",
    "output_file = \"data/chess_output.json\"\n",
    "num_samples = 100  # Number of samples to process from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MATE_DATASET\n",
    "\n",
    "Load the chess move evaluation dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MATE_DATASET in streaming mode...\")\n",
    "dataset_stream = load_dataset(\n",
    "    \"OutFlankShu/MATE_DATASET\", \n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Convert to list, handling errors gracefully\n",
    "dataset = []\n",
    "skipped = 0\n",
    "for i, sample in enumerate(dataset_stream):\n",
    "    try:\n",
    "        # Validate sample has required fields\n",
    "        if 'instruction' in sample and 'input' in sample and 'output' in sample:\n",
    "            dataset.append(sample)\n",
    "        else:\n",
    "            skipped += 1\n",
    "            print(f\"Skipping record {i}: missing required fields\")\n",
    "        \n",
    "        # Stop once we have enough samples (load extra in case of more errors)\n",
    "        if len(dataset) >= num_samples:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        print(f\"Skipping corrupted record {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} valid samples (skipped {skipped} corrupted records)\")\n",
    "print(f\"Will process {min(num_samples, len(dataset))} samples\")\n",
    "\n",
    "# Show example\n",
    "if len(dataset) > 0:\n",
    "    print(\"\\nExample sample:\")\n",
    "    print(f\"Instruction: {dataset[0]['instruction']}\")\n",
    "    print(f\"Input: {dataset[0]['input'][:200]}...\")\n",
    "    print(f\"Output: {dataset[0]['output']}\")\n",
    "else:\n",
    "    print(\"\\nWarning: No valid samples loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Conversation Template\n",
    "\n",
    "Format the chess evaluation task as a conversation for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(instruction, input_text):\n",
    "    \"\"\"\n",
    "    Create a conversation format for chess move evaluation.\n",
    "    \n",
    "    Uses a classification-style prompt to minimize output tokens and logits,\n",
    "    which is optimal for knowledge distillation training.\n",
    "    \n",
    "    Args:\n",
    "        instruction: The task instruction (system message) - will be replaced\n",
    "        input_text: The chess position and candidate moves\n",
    "    \n",
    "    Returns:\n",
    "        List of message dictionaries for the chat template\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Classify the better move. Output format: MoveA or MoveB\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "\n",
    "neuron_config = MoENeuronConfig(\n",
    "    tp_degree=8,\n",
    "    batch_size=1,\n",
    "    max_context_length=512,  # Increased for longer chess descriptions\n",
    "    seq_len=1024,\n",
    "    on_device_sampling_config=OnDeviceSamplingConfig(do_sample=True, temperature=0.6, top_k=20, top_p=0.95),\n",
    "    enable_bucketing=False,\n",
    "    flash_decoding_enabled=False,\n",
    "    output_scores=True,\n",
    "    output_logits=True\n",
    ")\n",
    "\n",
    "config = Qwen3MoeInferenceConfig(\n",
    "    neuron_config,\n",
    "    load_config=load_pretrained_config(model_path),\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Save Model\n",
    "\n",
    "**Note**: This step takes 30-60 minutes on first run. Skip if model is already compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCompiling and saving model...\")\n",
    "model = NeuronQwen3MoeForCausalLM(model_path, config)\n",
    "model.compile(traced_model_path)\n",
    "tokenizer.save_pretrained(traced_model_path)\n",
    "print(\"Model compiled and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Compiled Model\n",
    "\n",
    "Load the pre-compiled model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading compiled model...\")\n",
    "model = NeuronQwen3MoeForCausalLM(traced_model_path)\n",
    "model.load(traced_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset and Generate Logits\n",
    "\n",
    "Process chess positions through the teacher model to generate move evaluation logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for idx in range(min(num_samples, len(dataset))):\n",
    "    sample = dataset[idx]\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nProcessing sample {idx + 1}/{num_samples}...\")\n",
    "        \n",
    "        # Create conversation from instruction and input\n",
    "        conversation = create_conversation(sample['instruction'], sample['input'])\n",
    "        \n",
    "        # Format with chat template\n",
    "        formatted_chat = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(formatted_chat, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Create generation adapter (must be inside loop)\n",
    "        generation_model = HuggingFaceGenerationAdapter(model)\n",
    "        \n",
    "        # Generate with logits\n",
    "        outputs = generation_model.generate(\n",
    "            inputs.input_ids,\n",
    "            generation_config=generation_config,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_length=model.config.neuron_config.max_length,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            output_logits=True\n",
    "        )\n",
    "        \n",
    "        # Extract generated text\n",
    "        generated_tokens = outputs.sequences[0]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        \n",
    "        # Extract and filter logits\n",
    "        token_logits_list = []\n",
    "        for logits in outputs.scores:\n",
    "            finite_mask = torch.isfinite(logits[0])\n",
    "            finite_indices = torch.nonzero(finite_mask).squeeze().tolist()\n",
    "            finite_logits = logits[0][finite_mask]\n",
    "            \n",
    "            # Handle single value case\n",
    "            if isinstance(finite_indices, int):\n",
    "                finite_indices = [finite_indices]\n",
    "            \n",
    "            token_info = {\n",
    "                'indices': finite_indices,\n",
    "                'logits': finite_logits.tolist()\n",
    "            }\n",
    "            token_logits_list.append(token_info)\n",
    "        \n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(f\"Expected: {sample['output']}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'instruction': sample['instruction'],\n",
    "            'input': sample['input'],\n",
    "            'expected_output': sample['output'],\n",
    "            'response': {\n",
    "                'generated_text': generated_text,\n",
    "                'token_logits': token_logits_list\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "        results.append({\n",
    "            'instruction': sample['instruction'],\n",
    "            'input': sample['input'],\n",
    "            'expected_output': sample['output'],\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing complete! Processed {len(results)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save the generated logits to JSON for use in distillation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "print(f\"Total samples: {len(results)}\")\n",
    "print(f\"Successful: {sum(1 for r in results if 'error' not in r)}\")\n",
    "print(f\"Errors: {sum(1 for r in results if 'error' in r)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_8_nxd_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
