{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Neuron Distillation Training\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will learn how to train a smaller \"student\" model using knowledge distillation on AWS Neuron hardware. This lab builds directly on Lab 0, where you generated teacher model logits from the Qwen3-30B-A3B model.\n",
    "\n",
    "Knowledge distillation allows you to compress a large, high-performance model into a smaller, more efficient model while retaining much of the original model's capabilities. The student model learns not just from the hard labels (POSITIVE, NEGATIVE, NEUTRAL), but from the full probability distributions (soft labels) produced by the teacher model.\n",
    "\n",
    "**Why Knowledge Distillation?**\n",
    "- **Cost Reduction**: Smaller models require fewer compute resources and lower inference costs\n",
    "- **Faster Inference**: Reduced model size means faster response times\n",
    "- **Better Generalization**: Learning from soft labels often produces models that generalize better than training on hard labels alone\n",
    "- **Deployment Flexibility**: Smaller models can be deployed on edge devices or in resource-constrained environments\n",
    "\n",
    "**Training Approach:**\n",
    "\n",
    "You will use a custom `KnowledgeDistillationTrainer` that combines two loss functions:\n",
    "1. **Hard Loss**: Standard cross-entropy loss with true labels (teaches the model the correct answers)\n",
    "2. **Soft Loss**: KL divergence between teacher and student logits (teaches the model the teacher's reasoning)\n",
    "\n",
    "The combined loss is: `total_loss = α × soft_loss + (1 - α) × hard_loss`\n",
    "\n",
    "Where α (alpha) controls the balance between learning from the teacher vs. learning from the labels.\n",
    "\n",
    "**Student Model:**\n",
    "\n",
    "You'll train [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B), a 600 million parameter model - 50x smaller than the 30B teacher model!\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Lab 0 with teacher logits saved to `data/output.json`\n",
    "- AWS Trainium based EC2 instance\n",
    "- AWS Neuron SDK installed\n",
    "- Sufficient disk space for model compilation and checkpoints (~30GB recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the student model weights\n",
    "\n",
    "First, download the student model weights from HuggingFace, using the HuggingFace CLI. The model detail page can be found here: [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download Qwen/Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Configure environment variables to optimize Neuron compilation and runtime performance for distributed training.\n",
    "\n",
    "**Environment Variables Explained:**\n",
    "\n",
    "- **NEURON_CC_FLAGS**: Compiler flags for the Neuron compiler\n",
    "  - `--model-type transformer`: Optimizes compilation for transformer architectures\n",
    "  - `--retry_failed_compilation`: Automatically retry if compilation fails (improves reliability)\n",
    "\n",
    "- **NEURON_FUSE_SOFTMAX**: Enable softmax fusion optimization (combines operations for better performance)\n",
    "\n",
    "- **NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS**: Maximum number of concurrent inference requests (3 provides good throughput/latency balance)\n",
    "\n",
    "- **MALLOC_ARENA_MAX**: Limits memory arenas to reduce memory fragmentation during training (important for long-running jobs)\n",
    "\n",
    "- **WORLD_SIZE**: Total number of processes for distributed training. Set to 8 to match the number of NeuronCores we'll use (2 processes × 4 cores each = 8 total)\n",
    "\n",
    "These settings are critical for stable, efficient training on Neuron hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from src.util import prettyprint_python\n",
    "\n",
    "# Set Neuron compilation flags\n",
    "os.environ['NEURON_CC_FLAGS'] = \"--model-type transformer --retry_failed_compilation\"\n",
    "os.environ['NEURON_FUSE_SOFTMAX'] = \"1\"\n",
    "os.environ['NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS'] = \"3\"\n",
    "os.environ['MALLOC_ARENA_MAX'] = \"64\"\n",
    "os.environ['WORLD_SIZE'] = \"8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Define the hyperparameters and settings for the distillation training job.\n",
    "\n",
    "**Training Parameters:**\n",
    "\n",
    "- **PROCESSES_PER_NODE**: Number of training processes to run in parallel (2 processes for distributed training)\n",
    "  - Each process will use TP_DEGREE NeuronCores\n",
    "  - Total NeuronCores used = PROCESSES_PER_NODE × TP_DEGREE = 2 × 2 = 4\n",
    "\n",
    "- **NUM_EPOCHS**: Number of complete passes through the training dataset (3 epochs)\n",
    "  - More epochs = more training, but risk of overfitting on small datasets\n",
    "\n",
    "- **TP_DEGREE**: Tensor parallelism degree (2 NeuronCores per process)\n",
    "  - Distributes model layers across multiple cores for memory efficiency\n",
    "  - For a 0.6B model, TP=2 is sufficient\n",
    "\n",
    "- **BS**: Batch size per device (1 sample at a time)\n",
    "  - Small batch size for memory efficiency during compilation\n",
    "  - Effective batch size = BS × GRADIENT_ACCUMULATION_STEPS = 1 × 16 = 16\n",
    "\n",
    "- **GRADIENT_ACCUMULATION_STEPS**: Number of steps to accumulate gradients before updating weights (16)\n",
    "  - Simulates larger batch sizes without increasing memory usage\n",
    "  - Provides more stable gradient estimates\n",
    "\n",
    "- **LOGGING_STEPS**: Log training metrics every N steps (1 = log every step for detailed monitoring)\n",
    "\n",
    "- **MODEL_NAME**: Hugging Face model identifier for the student model\n",
    "  - \"Qwen/Qwen3-0.6B\": 600M parameter model (50x smaller than the 30B teacher)\n",
    "\n",
    "- **OUTPUT_DIR**: Directory where trained model checkpoints will be saved\n",
    "\n",
    "- **MAX_STEPS**: Maximum training steps\n",
    "  - Set to 5 if NEURON_EXTRACT_GRAPHS_ONLY=1 (for graph extraction/compilation testing)\n",
    "  - Set to -1 otherwise (train for full NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "PROCESSES_PER_NODE = 2\n",
    "NUM_EPOCHS = 3\n",
    "TP_DEGREE = 2\n",
    "BS = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "LOGGING_STEPS = 1\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "OUTPUT_DIR = f\"{MODEL_NAME.split('/')[-1]}-finetuned\"\n",
    "\n",
    "# Set max steps based on environment\n",
    "MAX_STEPS = 5 if os.environ.get('NEURON_EXTRACT_GRAPHS_ONLY') == '1' else -1\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Max steps: {MAX_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KnowledgeDistillationTrainer Code\n",
    "\n",
    "Let's examine the custom `KnowledgeDistillationTrainer` class from `distill_neuron_torchrun.py`.\n",
    "\n",
    "This trainer extends the Optimum Neuron `NeuronTrainer` class to implement knowledge distillation. The key innovation is in the `compute_loss` method, which combines two types of losses:\n",
    "\n",
    "**Class Structure:**\n",
    "\n",
    "```python\n",
    "class KnowledgeDistillationTrainer(NeuronTrainer):\n",
    "    def __init__(self, temperature=4.0, alpha=0.7, *args, **kwargs):\n",
    "```\n",
    "\n",
    "**Hyperparameters:**\n",
    "- **temperature** (default=4.0): Controls the \"softness\" of probability distributions\n",
    "  - Higher temperature = softer distributions (more uniform probabilities)\n",
    "  - Softer distributions reveal more about the teacher's uncertainty and reasoning\n",
    "  - Typical range: 2.0-10.0\n",
    "\n",
    "- **alpha** (default=0.7): Balances soft loss vs. hard loss\n",
    "  - alpha=0.7 means 70% weight on teacher's soft labels, 30% on true labels\n",
    "  - Higher alpha = more emphasis on mimicking the teacher\n",
    "  - Lower alpha = more emphasis on getting correct answers\n",
    "  - Typical range: 0.5-0.9\n",
    "\n",
    "The following cell displays the complete class implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint_python(\"src/distill_neuron_torchrun.py\", line_numbers=True, line_range=(35, 78))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Methods of KnowledgeDistillationTrainer\n",
    "\n",
    "The `compute_loss` method is the core of the knowledge distillation process. Let's break down how it works:\n",
    "\n",
    "**Step 1: Generate Student Predictions**\n",
    "```python\n",
    "student_outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "student_logits = student_outputs.logits\n",
    "```\n",
    "Run the student model to get its predictions (logits) for the input text.\n",
    "\n",
    "**Step 2: Compute Hard Loss**\n",
    "```python\n",
    "hard_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), inputs['labels'].view(-1))\n",
    "```\n",
    "Standard cross-entropy loss comparing student predictions to true labels (POSITIVE/NEGATIVE/NEUTRAL).\n",
    "This ensures the student learns to produce correct answers.\n",
    "\n",
    "**Step 3: Compute Soft Loss**\n",
    "```python\n",
    "student_soft = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
    "teacher_soft = F.softmax(inputs['teacher_logits'] / self.temperature, dim=-1)\n",
    "soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (self.temperature ** 2)\n",
    "```\n",
    "- Apply temperature scaling to both student and teacher logits (makes distributions softer)\n",
    "- Compute KL divergence: measures how different the student's distribution is from the teacher's\n",
    "- Multiply by temperature² to maintain gradient scale (mathematical requirement of distillation)\n",
    "\n",
    "**Step 4: Combine Losses**\n",
    "```python\n",
    "total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss\n",
    "```\n",
    "Weighted combination of soft and hard losses.\n",
    "\n",
    "**Example:**\n",
    "For input \"This phone's battery life is absolutely amazing!\":\n",
    "- Hard loss: Penalizes if student doesn't predict \"POSITIVE\"\n",
    "- Soft loss: Penalizes if student's confidence distribution differs from teacher's (e.g., teacher might be 95% confident POSITIVE, 4% NEUTRAL, 1% NEGATIVE)\n",
    "\n",
    "The following cell displays the complete `compute_loss` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint_python(\"src/distill_neuron_torchrun.py\", line_numbers=True, line_range=(41, 78))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Distillation Training\n",
    "\n",
    "Execute the distributed training job using `torchrun`, PyTorch's distributed training launcher.\n",
    "\n",
    "**Why torchrun?**\n",
    "- Manages multiple training processes automatically\n",
    "- Sets up distributed communication between processes\n",
    "- Handles process synchronization and fault tolerance\n",
    "\n",
    "**Command Structure:**\n",
    "\n",
    "```bash\n",
    "torchrun --nproc_per_node 2 src/distill_neuron_torchrun.py [training args]\n",
    "```\n",
    "\n",
    "**Key Arguments:**\n",
    "\n",
    "- **--nproc_per_node**: Number of processes to launch (2 = data parallel training across 2 processes)\n",
    "- **--model_id**: Student model to train (Qwen/Qwen3-0.6B)\n",
    "- **--num_train_epochs**: Number of complete passes through the dataset (3)\n",
    "- **--do_train**: Enable training mode\n",
    "- **--max_steps**: Maximum training steps (-1 = train for full epochs, 5 = quick test)\n",
    "- **--per_device_train_batch_size**: Batch size per device (1)\n",
    "- **--gradient_accumulation_steps**: Accumulate gradients over 16 steps (effective batch size = 16)\n",
    "- **--learning_rate**: Optimizer learning rate (1e-4 = 0.0001, conservative for distillation)\n",
    "- **--bf16**: Use BFloat16 precision (faster training, lower memory, minimal accuracy loss)\n",
    "- **--tensor_parallel_size**: Distribute model across 2 NeuronCores per process\n",
    "- **--warmup_steps**: Gradually increase learning rate over first 5 steps (stabilizes training)\n",
    "- **--pipeline_parallel_size**: No pipeline parallelism (1 = disabled)\n",
    "- **--logging_steps**: Log metrics every step for detailed monitoring\n",
    "- **--output_dir**: Where to save model checkpoints\n",
    "- **--overwrite_output_dir**: Overwrite existing checkpoints if present\n",
    "\n",
    "**Training Process:**\n",
    "\n",
    "1. **Compilation Phase** (first run only, ~20-30 minutes):\n",
    "   - Neuron compiler optimizes the model for Trainium hardware\n",
    "   - Generates NEFF (Neuron Executable File Format) files\n",
    "   - Cached for subsequent runs\n",
    "\n",
    "2. **Training Phase** (~10-15 minutes):\n",
    "   - Loads teacher logits from `data/output1.json`\n",
    "   - Trains student model using knowledge distillation\n",
    "   - Saves checkpoints to OUTPUT_DIR\n",
    "\n",
    "3. **Final Model** saved to `./final_distilled_model`\n",
    "\n",
    "**Monitoring Training:**\n",
    "- Watch for loss values decreasing over time\n",
    "- Soft loss and hard loss are logged separately\n",
    "- Training is complete when you see \"Training completed\" message\n",
    "\n",
    "**Note:** The first run will take significantly longer due to compilation. Be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Build the torchrun command\n",
    "cmd = [\n",
    "    \"torchrun\",\n",
    "    \"--nproc_per_node\", str(PROCESSES_PER_NODE),\n",
    "    \"src/distill_neuron_torchrun.py\",\n",
    "    \"--model_id\", MODEL_NAME,\n",
    "    \"--num_train_epochs\", str(NUM_EPOCHS),\n",
    "    \"--do_train\",\n",
    "    \"--max_steps\", str(MAX_STEPS),\n",
    "    \"--per_device_train_batch_size\", str(BS),\n",
    "    \"--gradient_accumulation_steps\", str(GRADIENT_ACCUMULATION_STEPS),\n",
    "    \"--learning_rate\", \"1e-4\",\n",
    "    \"--bf16\",\n",
    "    \"--tensor_parallel_size\", str(TP_DEGREE),\n",
    "    \"--warmup_steps\", \"5\",\n",
    "    \"--pipeline_parallel_size\", \"1\",\n",
    "    \"--logging_steps\", str(LOGGING_STEPS),\n",
    "    \"--output_dir\", OUTPUT_DIR,\n",
    "    \"--overwrite_output_dir\"\n",
    "]\n",
    "\n",
    "print(\"Running command:\")\n",
    "print(\" \".join(cmd))\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Execute the command\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(\"STDOUT:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"\\nSTDERR:\")\n",
    "    print(result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results and Next Steps\n",
    "\n",
    "Congratulations on completing the knowledge distillation training!\n",
    "\n",
    "**What You've Accomplished:**\n",
    "\n",
    "1. ✅ Trained a 0.6B parameter student model using knowledge from a 30B parameter teacher\n",
    "2. ✅ Achieved 50x model size reduction while retaining much of the teacher's performance\n",
    "3. ✅ Learned to use distributed training on AWS Trainium with Neuron SDK\n",
    "4. ✅ Implemented custom knowledge distillation loss combining soft and hard targets\n",
    "\n",
    "**Model Outputs:**\n",
    "\n",
    "Your trained model is saved in two locations:\n",
    "- **Checkpoints**: `{OUTPUT_DIR}/` - Contains intermediate training checkpoints\n",
    "- **Final Model**: `./final_distilled_model/` - The completed distilled model ready for deployment\n",
    "\n",
    "**Evaluating Your Model:**\n",
    "\n",
    "To test your distilled model's performance:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the distilled model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final_distilled_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./final_distilled_model\")\n",
    "\n",
    "# Test on a sample\n",
    "test_text = \"This phone's battery life is absolutely amazing!\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "**Expected Benefits of Distillation:**\n",
    "\n",
    "The student model (0.6B parameters) is approximately 50x smaller than the teacher model (30B parameters), which typically translates to:\n",
    "- **Reduced Model Size**: Significantly smaller memory footprint for deployment\n",
    "- **Faster Inference**: Fewer parameters mean faster forward passes\n",
    "- **Lower Costs**: Reduced compute requirements for inference\n",
    "- **Accuracy Trade-off**: Some performance loss is expected, but distillation helps retain more capability than training from scratch\n",
    "\n",
    "**Note:** Actual performance metrics will depend on your specific dataset, training configuration, and evaluation criteria. We recommend benchmarking your distilled model against the teacher on your test set to quantify the accuracy/efficiency trade-off.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. **Deploy for Inference**: Use the distilled model with AWS Inferentia for cost-effective production inference\n",
    "2. **Fine-tune Further**: Continue training on domain-specific data to improve performance\n",
    "3. **Experiment with Hyperparameters**:\n",
    "   - Try different temperature values (2.0-10.0)\n",
    "   - Adjust alpha to balance soft vs. hard loss\n",
    "   - Increase training epochs for better convergence\n",
    "4. **Quantization**: Apply INT8 quantization for additional 4x size reduction\n",
    "5. **Benchmark**: Compare inference latency and accuracy against the teacher model\n",
    "\n",
    "**Troubleshooting:**\n",
    "\n",
    "If training failed or results are poor:\n",
    "- Check that `data/output1.json` contains valid teacher logits from Lab 0\n",
    "- Verify sufficient disk space for compilation artifacts (~30GB)\n",
    "- Review CloudWatch logs for detailed error messages\n",
    "- Try reducing batch size or sequence length if running out of memory\n",
    "- Ensure Neuron SDK version compatibility with the model\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- [AWS Neuron Documentation](https://awsdocs-neuron.readthedocs-hosted.com/)\n",
    "- [Optimum Neuron GitHub](https://github.com/huggingface/optimum-neuron)\n",
    "- [Knowledge Distillation Paper](https://arxiv.org/abs/1503.02531) (Hinton et al.)\n",
    "- [Qwen3 Model Card](https://huggingface.co/Qwen/Qwen3-0.6B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_7",
   "language": "python",
   "name": "aws_neuronx_venv_pytorch_2_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
