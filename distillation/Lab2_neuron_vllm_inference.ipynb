{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation\n",
    "\n",
    "After completing the fine-tuning process, the next step is to compile the trained model for AWS Trainium inference using the Hugging Face Optimum Neuron toolchain.\n",
    "Neuron compilation optimizes the model graph and converts it into a Neuron Executable File Format (NEFF), enabling efficient execution on NeuronCores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum-cli export neuron \\\n",
    "  --model \"Qwen/Qwen3-0.6B\" \\\n",
    "  --task text-generation \\\n",
    "  --sequence_length 512 \\\n",
    "  --batch_size 1 \\\n",
    "  /home/ubuntu/environment/ml/qwen/compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We will install the Optimum Neuron vllm library.  Then, run inference using the compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optimum-neuron[vllm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can run the batch inference example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\n",
    "    model=\"/home/ubuntu/environment/ml/qwen/compiled_model\", #local compiled model\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=2048,\n",
    "    device=\"neuron\",\n",
    "    tensor_parallel_size=2,\n",
    "    override_neuron_config={})\n",
    "\n",
    "def create_conversation(sample):\n",
    "    return f\"\"\"<|im_start|>system\n",
    "    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\n",
    "    <|im_start|>user\n",
    "    {sample}\n",
    "    <|im_start|>assistant\"\"\"\n",
    "\n",
    "prompts = []\n",
    "with open('datasets.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            prompts.append( create_conversation(line.strip()) )\n",
    "print(prompts)\n",
    "sampling_params = SamplingParams(max_tokens=2048, temperature=0.8)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\n\\n Generated text: {generated_text!r} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start an inference endpoint on the current instance, like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model=\"/home/ubuntu/environment/ml/qwen/compiled_model\" \\\n",
    "    --max-num-seqs=1 \\\n",
    "    --max-model-len=512 \\\n",
    "    --tensor-parallel-size=2 \\\n",
    "    --port=8080 \\\n",
    "    --device \"neuron\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And query the inference endpoint like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl 127.0.0.1:8080/v1/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -X POST \\\n",
    "    -d '{\"prompt\":\"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant\", \"temperature\": 0.8, \"max_tokens\":128}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
