{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Neuron vLLM Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will learn how to deploy and run inference with your distilled model using vLLM on AWS Neuron hardware. This lab builds on Lab 1, where you trained a smaller student model using knowledge distillation.\n",
    "\n",
    "vLLM is a high-throughput and memory-efficient inference engine for large language models. The Optimum Neuron integration provides seamless support for running vLLM on AWS Trainium and Inferentia accelerators, enabling cost-effective and high-performance inference.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- How to compile your distilled model for Neuron inference\n",
    "- How to set up vLLM with Optimum Neuron for batch inference\n",
    "- How to run sentiment classification inference on your trained model\n",
    "- How to deploy an OpenAI-compatible API server for production use\n",
    "\n",
    "**Key Benefits of vLLM on Neuron:**\n",
    "- **High Throughput**: Optimized for serving multiple requests efficiently\n",
    "- **Memory Efficiency**: Advanced memory management for large models\n",
    "- **Hardware Acceleration**: Native support for AWS Trainium/Inferentia\n",
    "- **API Compatibility**: OpenAI-compatible API for easy integration\n",
    "- **Cost Optimization**: Leverage AWS Neuron hardware for cost-effective inference\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Lab 1 with a trained distilled model\n",
    "- AWS Trainium-based EC2 instance\n",
    "- Sufficient disk space for model compilation artifacts\n",
    "\n",
    "## Copy Tokenizer\n",
    "\n",
    "First, we need to copy the tokenizer from the original model to our compilation directory. The tokenizer is essential for converting text inputs into tokens that the model can process, and for decoding the model's token outputs back into human-readable text.\n",
    "\n",
    "**Why Copy the Tokenizer?**\n",
    "- The distilled model uses the same vocabulary as the original Qwen3-0.6B model\n",
    "- vLLM requires the tokenizer to be co-located with the compiled model\n",
    "- This ensures consistent tokenization between training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for 'Qwen/Qwen3-0.6B' saved to '/home/ubuntu/environment/ml/qwen/compiled_model'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Specify the name of the Hugging Face model\n",
    "model_name = \"Qwen/Qwen3-0.6B\" \n",
    "\n",
    "# 2. Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. Define the local directory where you want to save the tokenizer\n",
    "local_directory = \"/home/ubuntu/environment/ml/qwen/compiled_model\"\n",
    "\n",
    "# 4. Save the tokenizer to the local directory\n",
    "tokenizer.save_pretrained(local_directory)\n",
    "\n",
    "print(f\"Tokenizer for '{model_name}' saved to '{local_directory}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation for Neuron Inference\n",
    "\n",
    "After completing the distillation training process, the next step is to compile the trained model for AWS Trainium inference using the Hugging Face Optimum Neuron toolchain.\n",
    "\n",
    "**What is Neuron Compilation?**\n",
    "\n",
    "Neuron compilation is a critical optimization step that:\n",
    "1. **Analyzes the Model Graph**: Examines the PyTorch model architecture and operations\n",
    "2. **Applies Hardware Optimizations**: Optimizes operations specifically for Neuron hardware\n",
    "3. **Generates NEFF Files**: Creates Neuron Executable File Format (NEFF) files for efficient execution\n",
    "4. **Enables Tensor Parallelism**: Distributes model layers across multiple NeuronCores\n",
    "5. **Optimizes Memory Usage**: Reduces memory footprint and improves throughput\n",
    "\n",
    "**Compilation Parameters:**\n",
    "\n",
    "- **--model**: Path to your distilled model from Lab 1\n",
    "- **--task**: Specifies the model task (text-generation for causal language models)\n",
    "- **--sequence_length**: Maximum sequence length the model can handle (2048 tokens)\n",
    "- **--batch_size**: Number of sequences to process simultaneously (1 for this example)\n",
    "- **Output Directory**: Where the compiled model artifacts will be saved\n",
    "\n",
    "**Expected Compilation Time:**\n",
    "- First compilation: ~5-10 minutes for a 0.6B model\n",
    "- Subsequent runs: Uses cached compilation if parameters haven't changed\n",
    "\n",
    "**Note:** The compilation process will generate detailed logs showing the optimization steps. This is normal and indicates the compiler is working to optimize your model for Neuron hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:70: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private_kernels.blockwise_matmul_while'\n",
      "  warnings.warn(f\"Warning: {error}\")\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:33: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "WARNING:Neuron:Ignoring the following kwargs as they are not supported by neuron: dict_keys(['subfolder', 'torch_dtype'])\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding', 'token_generation']\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7329f3d0f7f0>, 'Ascending Ring PG Group')>\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-10-31 01:16:22.885: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n",
      "INFO:Neuron:Generating 1 hlos for key: context_encoding\n",
      "INFO:Neuron:Started loading module context_encoding\n",
      "INFO:Neuron:Finished loading module context_encoding in 0.1002206802368164 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding, input example shape = torch.Size([1, 2048])\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:507: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:299: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:262: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 2048]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for context_encoding in 2.347781181335449 seconds, input example shape = torch.Size([1, 2048])\n",
      "INFO:Neuron:Generating 1 hlos for key: token_generation\n",
      "INFO:Neuron:Started loading module token_generation\n",
      "INFO:Neuron:Finished loading module token_generation in 0.09135556221008301 seconds\n",
      "INFO:Neuron:generating HLO: token_generation, input example shape = torch.Size([1, 1])\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:507: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:299: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:262: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for token_generation in 1.739989995956421 seconds, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Generated all HLOs in 4.321739912033081 seconds\n",
      "INFO:Neuron:Starting compilation for the priority HLO\n",
      "INFO:Neuron:'token_generation' is the priority model with bucket rank 0\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "neuronxcc-2.21.18209.0+043b1bf7/MODULE_87301aa5994aa3ec3488+ac10809c/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.21.18209.0+043b1bf7/MODULE_87301aa5994aa3ec3488+ac10809c/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.21.18209.0+043b1bf7/MODULE_87301aa5994aa3ec3488+ac10809c/model.log not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "2025-10-31 01:16:27.000923:  56624  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxd_model/token_generation/_tp0_bk0/model.MODULE_87301aa5994aa3ec3488+ac10809c.hlo_module.pb --output /tmp/nxd_model/token_generation/_tp0_bk0/model.MODULE_87301aa5994aa3ec3488+ac10809c.neff --target=trn2 --auto-cast=none --model-type=transformer --tensorizer-options=--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma  -O2 --lnc=2 --logfile=/tmp/nxd_model/token_generation/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n",
      "....Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done compilation for the priority HLO in 79.36669445037842 seconds\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs in 6.9736738204956055 seconds\n",
      "INFO:Neuron:Starting compilation for all HLOs\n",
      "INFO:Neuron:Neuron compiler flags: --auto-cast=none --model-type=transformer --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' -O2  --lnc=2 --target trn2 --logfile=/tmp/nxd_model/context_encoding/_tp0_bk0/log-neuron-cc.txt\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:245: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "neuronxcc-2.21.18209.0+043b1bf7/MODULE_d520c8be8d8ce950a101+a32116a7/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.21.18209.0+043b1bf7/MODULE_d520c8be8d8ce950a101+a32116a7/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.21.18209.0+043b1bf7/MODULE_d520c8be8d8ce950a101+a32116a7/model.log not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "2025-10-31 01:17:54.000179:  56624  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxd_model/context_encoding/_tp0_bk0/model.MODULE_d520c8be8d8ce950a101+a32116a7.hlo_module.pb --output /tmp/nxd_model/context_encoding/_tp0_bk0/model.MODULE_d520c8be8d8ce950a101+a32116a7.neff --target=trn2 --auto-cast=none --model-type=transformer --tensorizer-options=--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma  -O2 --lnc=2 --logfile=/tmp/nxd_model/context_encoding/_tp0_bk0/log-neuron-cc.txt --verbose=35\n",
      "........Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Finished Compilation for all HLOs in 141.5979459285736 seconds\n",
      "..Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done preparing weight layout transformation\n",
      "INFO:Neuron:Finished building model in 271.1505181789398 seconds\n",
      "Configuration saved in /home/ubuntu/environment/ml/qwen/compiled_model/neuron_config.json\n",
      "Model and configuration files saved in /home/ubuntu/environment/ml/qwen/compiled_model\n",
      "Configuration saved in /home/ubuntu/environment/ml/qwen/compiled_model/neuron_config.json\n",
      "INFO:Neuron:Sharding Weights for ranks: 0...0\n",
      "[2025-10-31 01:20:54.207: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7329f3d0f7f0>, 'Ascending Ring PG Group')>\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-10-31 01:20:54.208: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:642: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "INFO:Neuron:Done Sharding weights in 1.7758041390006838\n",
      "No tokenizer found while exporting /home/ubuntu/environment/distillation/Qwen3-0.6B-finetuned.\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export neuron \\\n",
    "  --model \"/home/ubuntu/environment/distillation/Qwen3-0.6B-finetuned\" \\\n",
    "  --task text-generation \\\n",
    "  --sequence_length 2048 \\\n",
    "  --batch_size 1 \\\n",
    "  /home/ubuntu/environment/ml/qwen/compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup vLLM for Neuron Inference\n",
    "\n",
    "Now we'll install the Optimum Neuron vLLM library and run inference using our compiled distilled model.\n",
    "\n",
    "**What is vLLM?**\n",
    "\n",
    "vLLM (Versatile Large Language Model) is a high-performance inference engine designed for:\n",
    "- **High Throughput**: Serves multiple requests efficiently with advanced batching\n",
    "- **Memory Efficiency**: Uses PagedAttention and other optimizations to reduce memory usage\n",
    "- **Hardware Acceleration**: Native support for GPUs, TPUs, and AWS Neuron accelerators\n",
    "- **API Compatibility**: Provides OpenAI-compatible APIs for easy integration\n",
    "\n",
    "**Optimum Neuron Integration:**\n",
    "\n",
    "The `optimum-neuron[vllm]` package provides:\n",
    "- Seamless integration between vLLM and AWS Neuron hardware\n",
    "- Automatic handling of model sharding across NeuronCores\n",
    "- Optimized attention mechanisms for Trainium/Inferentia\n",
    "- Support for various model architectures including Qwen3\n",
    "\n",
    "**Installation Note:**\n",
    "The installation may take a few minutes as it includes the full vLLM package with Neuron optimizations and all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: optimum-neuron[vllm] in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: transformers~=4.55.4 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from optimum-neuron[vllm]) (4.55.4)\n",
      "Requirement already satisfied: accelerate==1.8.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from optimum-neuron[vllm]) (1.8.1)\n",
      "Requirement already satisfied: optimum~=1.24.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from optimum-neuron[vllm]) (1.24.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.31.4 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from optimum-neuron[vllm]) (0.36.0)\n",
      "Requirement already satisfied: numpy<=1.26.4,>=1.22.2 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from optimum-neuron[vllm]) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4,>=3.20.3 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from optimum-neuron[vllm]) (3.20.3)\n",
      "Collecting vllm==0.10.2 (from optimum-neuron[vllm])\n",
      "  Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (2.8.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (0.6.2)\n",
      "Requirement already satisfied: regex in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (2025.10.23)\n",
      "Collecting cachetools (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading cachetools-6.2.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting sentencepiece (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Using cached sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (4.67.1)\n",
      "Collecting blake3 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading blake3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting py-cpuinfo (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (0.21.4)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (0.116.2)\n",
      "Requirement already satisfied: aiohttp in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (3.12.15)\n",
      "Collecting openai>=1.99.1 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading openai-2.6.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pydantic>=2.11.7 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (2.11.9)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (0.23.1)\n",
      "Requirement already satisfied: pillow in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (11.3.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer==0.11.3 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.11 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading outlines_core-0.2.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: lark==1.2.2 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (1.2.2)\n",
      "Collecting xgrammar==0.1.23 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading xgrammar-0.1.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (3.19.1)\n",
      "Collecting partial-json-parser (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (27.1.0)\n",
      "Collecting msgspec (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting einops (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.11.0 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.19.0 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: cloudpickle in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (3.1.1)\n",
      "Collecting watchfiles (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading watchfiles-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (3.3.0)\n",
      "Requirement already satisfied: scipy in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (1.12.0)\n",
      "Collecting ninja (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading pybase64-1.4.2-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading cbor2-5.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting setproctitle (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading setproctitle-1.3.7-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting numba==0.61.2 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading ray-2.51.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting torchaudio==2.8.0 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading torchaudio-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: torchvision==0.23.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from vllm==0.10.2->optimum-neuron[vllm]) (0.23.0)\n",
      "Collecting xformers==0.0.32.post1 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting frozendict (from compressed-tensors==0.11.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading frozendict-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Collecting astor (from depyf==0.19.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from depyf==0.19.0->vllm==0.10.2->optimum-neuron[vllm]) (0.4.0)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from triton==3.4.0->torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (80.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from huggingface-hub>=0.31.4->optimum-neuron[vllm]) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (0.48.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from pydantic>=2.11.7->vllm==0.10.2->optimum-neuron[vllm]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from pydantic>=2.11.7->vllm==0.10.2->optimum-neuron[vllm]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from pydantic>=2.11.7->vllm==0.10.2->optimum-neuron[vllm]) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from starlette<0.49.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (4.10.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (1.3.1)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading fastapi_cli-0.0.14-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (0.28.1)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting typer>=0.15.1 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading rignore-0.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting sentry-sdk>=2.20.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading sentry_sdk-2.43.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: certifi in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (3.0.2)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm]) (4.25.1)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm]) (0.27.1)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.99.1->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=1.99.1->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading jiter-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting click!=8.3.0,>=7.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading cupy_cuda12x-13.6.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from requests>=2.26.0->vllm==0.10.2->optimum-neuron[vllm]) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from requests>=2.26.0->vllm==0.10.2->optimum-neuron[vllm]) (2.5.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (14.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm]) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (1.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading httptools-0.7.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading uvloop-0.22.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from aiohttp->vllm==0.10.2->optimum-neuron[vllm]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from aiohttp->vllm==0.10.2->optimum-neuron[vllm]) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from aiohttp->vllm==0.10.2->optimum-neuron[vllm]) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from aiohttp->vllm==0.10.2->optimum-neuron[vllm]) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from aiohttp->vllm==0.10.2->optimum-neuron[vllm]) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from aiohttp->vllm==0.10.2->optimum-neuron[vllm]) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from aiohttp->vllm==0.10.2->optimum-neuron[vllm]) (1.20.1)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting soundfile>=0.12.1 (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm]) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm]) (2.23)\n",
      "Collecting soxr>=0.5.0 (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2->optimum-neuron[vllm])\n",
      "  Downloading soxr-1.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl (436.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n",
      "Downloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
      "Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m176.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines_core-0.2.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading xgrammar-0.1.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
      "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
      "Downloading fastapi_cli-0.0.14-py3-none-any.whl (11 kB)\n",
      "Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl (19 kB)\n",
      "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-2.6.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (359 kB)\n",
      "Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m239.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ray-2.51.0-cp310-cp310-manylinux2014_x86_64.whl (71.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m71.2/71.2 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (406 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading rich_toolkit-0.15.1-py3-none-any.whl (29 kB)\n",
      "Downloading rignore-0.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m959.6/959.6 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.43.0-py2.py3-none-any.whl (400 kB)\n",
      "Downloading tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading httptools-0.7.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (440 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading uvloop-0.22.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m165.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading blake3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (387 kB)\n",
      "Downloading cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Downloading cbor2-5.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (247 kB)\n",
      "Downloading cupy_cuda12x-13.6.0-cp310-cp310-manylinux2014_x86_64.whl (112.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m112.2/112.2 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading frozendict-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pybase64-1.4.2-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (68 kB)\n",
      "Using cached sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "Downloading setproctitle-1.3.7-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (31 kB)\n",
      "Installing collected packages: py-cpuinfo, fastrlock, websockets, uvloop, soxr, shellingham, setproctitle, sentry-sdk, sentencepiece, rignore, python-multipart, python-dotenv, pycountry, pybase64, partial-json-parser, outlines_core, opencv-python-headless, ninja, msgspec, msgpack, llvmlite, llguidance, jiter, interegular, httptools, gguf, frozendict, einops, dnspython, distro, diskcache, cupy-cuda12x, click, cbor2, cachetools, blake3, astor, uvicorn, tiktoken, soundfile, numba, email-validator, depyf, watchfiles, typer, rich-toolkit, pydantic-extra-types, openai-harmony, lm-format-enforcer, xformers, torchaudio, ray, prometheus-fastapi-instrumentator, openai, xgrammar, mistral_common, fastapi-cloud-cli, fastapi-cli, compressed-tensors, vllm\n",
      "\u001b[2K  Attempting uninstall: llvmlite0m\u001b[90m\u001b[0m \u001b[32m19/60\u001b[0m [msgpack]ython-headless]\n",
      "\u001b[2K    Found existing installation: llvmlite 0.45.0\u001b[0m \u001b[32m19/60\u001b[0m [msgpack]\n",
      "\u001b[2K    Uninstalling llvmlite-0.45.0:0m\u001b[0m \u001b[32m19/60\u001b[0m [msgpack]\n",
      "\u001b[2K      Successfully uninstalled llvmlite-0.45.0\u001b[0m \u001b[32m19/60\u001b[0m [msgpack]\n",
      "\u001b[2K  Attempting uninstall: click[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m31/60\u001b[0m [cupy-cuda12x]\n",
      "\u001b[2K    Found existing installation: click 8.3.0\u001b[0m \u001b[32m31/60\u001b[0m [cupy-cuda12x]\n",
      "\u001b[2K    Uninstalling click-8.3.0:[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m31/60\u001b[0m [cupy-cuda12x]\n",
      "\u001b[2K      Successfully uninstalled click-8.3.0m\u001b[0m \u001b[32m31/60\u001b[0m [cupy-cuda12x]\n",
      "\u001b[2K  Attempting uninstall: numba\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m37/60\u001b[0m [uvicorn]]\n",
      "\u001b[2K    Found existing installation: numba 0.62.00m\u001b[0m \u001b[32m37/60\u001b[0m [uvicorn]\n",
      "\u001b[2K    Uninstalling numba-0.62.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m40/60\u001b[0m [numba]\n",
      "\u001b[2K      Successfully uninstalled numba-0.62.0m\u001b[90m\u001b[0m \u001b[32m40/60\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m60/60\u001b[0m [vllm]0m [vllm]0m [mistral_common]\n",
      "\u001b[1A\u001b[2KSuccessfully installed astor-0.8.1 blake3-1.0.8 cachetools-6.2.1 cbor2-5.7.1 click-8.2.1 compressed-tensors-0.11.0 cupy-cuda12x-13.6.0 depyf-0.19.0 diskcache-5.6.3 distro-1.9.0 dnspython-2.8.0 einops-0.8.1 email-validator-2.3.0 fastapi-cli-0.0.14 fastapi-cloud-cli-0.3.1 fastrlock-0.8.3 frozendict-2.4.6 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 jiter-0.11.1 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 mistral_common-1.8.5 msgpack-1.1.2 msgspec-0.19.0 ninja-1.13.0 numba-0.61.2 openai-2.6.1 openai-harmony-0.0.4 opencv-python-headless-4.11.0.86 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 py-cpuinfo-9.0.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.6 python-dotenv-1.2.1 python-multipart-0.0.20 ray-2.51.0 rich-toolkit-0.15.1 rignore-0.7.2 sentencepiece-0.2.1 sentry-sdk-2.43.0 setproctitle-1.3.7 shellingham-1.5.4 soundfile-0.13.1 soxr-1.0.0 tiktoken-0.12.0 torchaudio-2.8.0 typer-0.20.0 uvicorn-0.38.0 uvloop-0.22.1 vllm-0.10.2 watchfiles-1.1.1 websockets-15.0.1 xformers-0.0.32.post1 xgrammar-0.1.23\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q optimum-neuron[vllm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference with Your Distilled Model\n",
    "\n",
    "Now we'll run batch inference using your distilled sentiment classification model. This example demonstrates how to:\n",
    "\n",
    "1. **Initialize the vLLM Engine**: Load your compiled model with appropriate configuration\n",
    "2. **Prepare Input Data**: Format text samples using the same conversation template from training\n",
    "3. **Run Batch Inference**: Process multiple samples efficiently in parallel\n",
    "4. **Analyze Results**: Compare the distilled model's predictions with expected sentiment classifications\n",
    "\n",
    "**vLLM Configuration Parameters:**\n",
    "\n",
    "- **model**: Path to your compiled Neuron model\n",
    "- **max_num_seqs**: Maximum number of sequences to process simultaneously (1 for this example)\n",
    "- **max_model_len**: Maximum sequence length the model can handle (2048 tokens)\n",
    "- **tensor_parallel_size**: Number of NeuronCores to use for tensor parallelism (2)\n",
    "\n",
    "**Conversation Template:**\n",
    "\n",
    "We use the same conversation format that was used during training:\n",
    "- **System Message**: Defines the sentiment classification task\n",
    "- **User Message**: Contains the text to classify\n",
    "- **Assistant Response**: Expected to be POSITIVE, NEGATIVE, or NEUTRAL\n",
    "\n",
    "**Expected Behavior:**\n",
    "\n",
    "Your distilled model should demonstrate:\n",
    "- **Faster Inference**: Significantly faster than the 30B teacher model\n",
    "- **Reasonable Accuracy**: Good sentiment classification despite being 50x smaller\n",
    "- **Consistent Format**: Responses in the expected POSITIVE/NEGATIVE/NEUTRAL format\n",
    "\n",
    "**Performance Comparison:**\n",
    "\n",
    "Compare the results with what you'd expect from the teacher model to evaluate the effectiveness of knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-31 01:38:28 [utils.py:328] non-default args: {'max_model_len': 2048, 'tensor_parallel_size': 2, 'max_num_seqs': 1, 'disable_log_stats': True, 'model': '/home/ubuntu/environment/ml/qwen/compiled_model'}\n",
      "INFO 10-31 01:38:28 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 10-31 01:38:28 [__init__.py:1815] Using max model len 2048\n",
      "INFO 10-31 01:38:28 [llm_engine.py:221] Initializing a V0 LLM engine (v0.10.2) with config: model='/home/ubuntu/environment/ml/qwen/compiled_model', speculative_config=None, tokenizer='/home/ubuntu/environment/ml/qwen/compiled_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/home/ubuntu/environment/ml/qwen/compiled_model, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Loading sharded checkpoint from /home/ubuntu/environment/ml/qwen/compiled_model/checkpoint/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-31 01:38:35 [__init__.py:3804] Current vLLM config is not set.\n",
      "WARNING 10-31 01:38:35 [__init__.py:3804] Current vLLM config is not set.\n",
      "INFO 10-31 01:38:35 [executor_base.py:114] # neuron blocks: 2, # CPU blocks: 0\n",
      "INFO 10-31 01:38:35 [executor_base.py:119] Maximum concurrency for 2048 tokens per request: 2.00x\n",
      "INFO 10-31 01:38:35 [llm_engine.py:420] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n",
      "INFO 10-31 01:38:35 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-31 01:38:35 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "['<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a8163e517b4fc09f7a0e32af60a3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e967191fcb442bb17515dc3bdacd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe weather is 72 degrees with partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe weather is 72 degrees with partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!\\n</think>assistant\\nassistant\\nI deeply regret purchasing this defective product.' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\n</think>assistant\\nThe battery life of this phone is absolutely amazing!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe defective product was returned as expected.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe team just won the championship - best day ever!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe staff at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\n</think>assistant\\nThe weather is 72 degrees with partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe flight was delayed for the third time this week.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\n</think>assistant\\nThe battery life of this phone is absolutely amazing!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\n</think>assistant\\nThe product contains defects not fully specified.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe staff exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe flight was delayed for the third time this week.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: \"\\nThe phone's battery life is absolutely amazing!\" \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe store will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe team just won the championship - best day ever!\\n<tool_call>\\n adapting to the circumstances\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team just won the championship - best day ever!\\nassistant\\nassistant\\nThe team' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\n</think>assistant\\nThe staff exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe weather is 72 degrees with partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe battery life of this smartphone exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe store will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe team just won the championship.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe weather is 72 degrees with partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nOur flight was delayed for the third time this week.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: \"\\nassistant\\nThe phone's battery life is absolutely amazing!\" \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe store will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe product contains defects as specified in the service contract.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe championship - best day ever!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe weather is 72 degrees with partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe repairs made last fiscal month had ended this week.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\n</think>assistant\\nThe package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: \"\\nassistant\\nThe phone's battery life is absolutely amazing!\" \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe store will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: \"\\nassistant\\nThis year's championship - best day ever!\" \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: \"\\n</think>user\\nI can't believe how rude the staff was today.\" \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe weather is 72 degrees with partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe flight was delayed for the third time this week.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe battery life of this phone is absolutely amazing!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\n</think>assistant\\nThe store will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe product contains parts worn by prolonged use.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\n.The fireworks fell silently into the night.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe staff exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nenses\\nThis package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe weather is 72 degrees with partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: \"\\nassistant\\nThis phone's battery life is absolutely amazing!\" \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe store will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe ceremony at this championship - best day ever!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe flight was delayed for the third time this week.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: \"\\nassistant\\nThe phone's battery life is absolutely amazing!\" \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe store will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe product contains parts that are not in good condition.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe team just won the championship - best day ever!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I can't believe how rude the staff was today.\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The weather is 72 degrees with partial clouds.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe forecast shows partial clouds.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    My flight was delayed for the third time this week.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe flight was delayed for the third time this week.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The package arrived on schedule as expected.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe package arrived on schedule as expected.' \n",
      "\n",
      "Prompt: \"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    This phone's battery life is absolutely amazing!\\n    <|im_start|>assistant\", \n",
      "\n",
      " Generated text: \"\\nassistant\\nThe phone's battery life is absolutely amazing!\" \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The store will be closed from 2PM to 4PM.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant will be closed from 2PM to 4PM.' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    I deeply regret purchasing this defective product.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe service at this restaurant exceeded all my expectations!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    Our team just won the championship - best day ever!\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe team just won the championship - best day ever!' \n",
      "\n",
      "Prompt: '<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The report contains data from the last fiscal year.\\n    <|im_start|>assistant', \n",
      "\n",
      " Generated text: '\\nassistant\\nThe report contains data from the last fiscal year.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\n",
    "    model=\"/home/ubuntu/environment/ml/qwen/compiled_model\", #local compiled model\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=2048,\n",
    "    tensor_parallel_size=2,)\n",
    "\n",
    "def create_conversation(sample):\n",
    "    return f\"\"\"<|im_start|>system\n",
    "    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\n",
    "    <|im_start|>user\n",
    "    {sample}\n",
    "    <|im_start|>assistant\"\"\"\n",
    "\n",
    "prompts = []\n",
    "with open('/home/ubuntu/environment/distillation/data/dataset.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            prompts.append( create_conversation(line.strip()) )\n",
    "print(prompts)\n",
    "sampling_params = SamplingParams(max_tokens=2048, temperature=0.8)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\n\\n Generated text: {generated_text!r} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Production API Server\n",
    "\n",
    "For production use cases, you can deploy your distilled model as an OpenAI-compatible API server. This allows you to integrate your sentiment classification model into applications using standard HTTP requests.\n",
    "\n",
    "**API Server Benefits:**\n",
    "\n",
    "- **OpenAI Compatibility**: Use the same API format as OpenAI's models\n",
    "- **HTTP Interface**: Easy integration with web applications and services\n",
    "- **Concurrent Requests**: Handle multiple requests simultaneously\n",
    "- **Production Ready**: Built-in request queuing and error handling\n",
    "- **Cost Effective**: Run your own model instead of paying per API call\n",
    "\n",
    "**Server Configuration:**\n",
    "\n",
    "- **--model**: Path to your compiled Neuron model\n",
    "- **--max-num-seqs**: Maximum concurrent sequences (1 for this configuration)\n",
    "- **--max-model-len**: Maximum sequence length (2048 tokens)\n",
    "- **--tensor-parallel-size**: NeuronCores to use (2 for optimal performance)\n",
    "- **--port**: HTTP port for the API server (8080)\n",
    "- **--device**: Specify \"neuron\" to use AWS Neuron hardware\n",
    "\n",
    "**Starting the Server:**\n",
    "\n",
    "Run the following command to start your API server. The server will be accessible at `http://localhost:8080` and provide OpenAI-compatible endpoints like `/v1/completions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model=\"/home/ubuntu/environment/ml/qwen/compiled_model\" \\\n",
    "    --max-num-seqs=1 \\\n",
    "    --max-model-len=2048 \\\n",
    "    --tensor-parallel-size=2 \\\n",
    "    --port=8080 \\\n",
    "    --device \"neuron\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the API Server\n",
    "\n",
    "Once your API server is running, you can test it using standard HTTP requests. The server provides OpenAI-compatible endpoints that accept the same request format.\n",
    "\n",
    "**API Endpoint:**\n",
    "- **URL**: `http://127.0.0.1:8080/v1/completions`\n",
    "- **Method**: POST\n",
    "- **Content-Type**: application/json\n",
    "\n",
    "**Request Parameters:**\n",
    "- **prompt**: The formatted conversation prompt (same format as training)\n",
    "- **temperature**: Controls randomness in generation (0.8 for balanced creativity)\n",
    "- **max_tokens**: Maximum number of tokens to generate (128 should be sufficient for sentiment labels)\n",
    "\n",
    "**Example Request:**\n",
    "\n",
    "The following curl command demonstrates how to query your sentiment classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl 127.0.0.1:8080/v1/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -X POST \\\n",
    "    -d '{\"prompt\":\"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant\", \"temperature\": 0.8, \"max_tokens\":128}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2 Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully completed the knowledge distillation workflow by deploying your distilled model for high-performance inference.\n",
    "\n",
    "**What You've Accomplished:**\n",
    "\n",
    "1.  **Model Compilation**: Optimized your distilled model for AWS Neuron hardware\n",
    "2.  **vLLM Integration**: Set up high-performance inference with Optimum Neuron\n",
    "3.  **Batch Processing**: Demonstrated efficient batch inference on sentiment classification\n",
    "4.  **API Deployment**: Deployed an OpenAI-compatible API server for production use\n",
    "\n",
    "**Performance Benefits Achieved:**\n",
    "\n",
    "- **50x Model Size Reduction**: From 30B parameters (teacher) to 0.6B parameters (student)\n",
    "- **Faster Inference**: Significantly reduced latency compared to the teacher model\n",
    "- **Cost Optimization**: Lower compute costs for production inference\n",
    "- **Hardware Acceleration**: Optimized execution on AWS Trainium/Inferentia\n",
    "\n",
    "**Production Considerations:**\n",
    "\n",
    "- **Scaling**: Use multiple instances behind a load balancer for higher throughput\n",
    "- **Monitoring**: Implement logging and metrics collection for production monitoring\n",
    "- **Security**: Add authentication and rate limiting for public APIs\n",
    "- **Optimization**: Fine-tune batch sizes and parallelism based on your workload\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. **Evaluate Performance**: Compare accuracy and speed against the teacher model\n",
    "2. **Integration**: Integrate the API into your applications\n",
    "3. **Optimization**: Experiment with different compilation settings for your use case\n",
    "4. **Scaling**: Deploy multiple instances for production workloads\n",
    "5. **Monitoring**: Set up comprehensive monitoring and alerting\n",
    "\n",
    "You now have a complete knowledge distillation pipeline from teacher model logit generation through production deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
