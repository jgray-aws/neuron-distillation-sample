{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Chess Move Evaluation - Knowledge Distillation Training\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will train a smaller \"student\" model to evaluate chess moves using knowledge distillation. This builds on Lab 0, where you generated teacher model logits from the Qwen3-30B-A3B model.\n",
    "\n",
    "**Task**: Train a student model to classify which chess move is better (MoveA or MoveB)\n",
    "\n",
    "**Why Knowledge Distillation for Chess?**\n",
    "- **Cost Reduction**: 50x smaller model (30B → 0.6B parameters)\n",
    "- **Faster Inference**: ~20-50x faster move evaluation\n",
    "- **Deployment Flexibility**: Can run on smaller instances or edge devices\n",
    "- **Maintained Performance**: Retains much of the teacher's chess understanding\n",
    "\n",
    "**Training Approach:**\n",
    "\n",
    "The `KnowledgeDistillationTrainer` combines two loss functions:\n",
    "1. **Hard Loss**: Cross-entropy with true labels (MoveA or MoveB)\n",
    "2. **Soft Loss**: KL divergence between teacher and student logits\n",
    "\n",
    "Combined loss: `total_loss = α × soft_loss + (1 - α) × hard_loss`\n",
    "\n",
    "Where α=0.7 means 70% weight on learning from teacher, 30% on correct answers.\n",
    "\n",
    "**Models:**\n",
    "- **Teacher**: Qwen3-30B-A3B (30 billion parameters)\n",
    "- **Student**: Qwen3-0.6B (600 million parameters)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Lab 0 with chess logits saved to `data/chess_output.json`\n",
    "- AWS Trainium instance (trn1.32xlarge recommended)\n",
    "- AWS Neuron SDK installed\n",
    "- Virtual environment: `/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Student Model\n",
    "\n",
    "Download the Qwen3-0.6B model weights from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q neuronx-distributed datasets optimum-neuron[training]==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10 files:   0%|                                 | 0/10 [00:00<?, ?it/s]Downloading 'tokenizer.json' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4.incomplete'\n",
      "Downloading 'model.safetensors' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/f47f71177f32bcd101b7573ec9171e6a57f4f4d31148d38e382306f42996874b.incomplete'\n",
      "Downloading 'README.md' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/a50b19e76f5274f9ec99f5a5d99873dca5bff25e.incomplete'\n",
      "\n",
      "README.md: 14.0kB [00:00, 51.0MB/s]\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/a50b19e76f5274f9ec99f5a5d99873dca5bff25e\n",
      "Downloading 'LICENSE' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/6634c8cc3133b3848ec74b9f275acaaa1ea618ab.incomplete'\n",
      "Downloading 'config.json' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/f5c3703b78ae2a478ae15b247e9f855e0ce2107b.incomplete'\n",
      "Downloading 'generation_config.json' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/20a8a9156fc8c3f25295ca067f61fdf120d517c5.incomplete'\n",
      "\n",
      "LICENSE: 0.00B [00:00, ?B/s]\u001b[ADownloading 'merges.txt' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.incomplete'\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 726/726 [00:00<00:00, 9.04MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/f5c3703b78ae2a478ae15b247e9f855e0ce2107b\n",
      "LICENSE: 11.3kB [00:00, 5.34MB/s]\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/6634c8cc3133b3848ec74b9f275acaaa1ea618ab\n",
      "\n",
      "generation_config.json: 100%|██████████████████| 239/239 [00:00<00:00, 2.56MB/s]\u001b[A\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/20a8a9156fc8c3f25295ca067f61fdf120d517c5\n",
      "\n",
      "merges.txt: 1.67MB [00:00, 102MB/s]\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7\n",
      "\n",
      "tokenizer.json:   0%|                               | 0.00/11.4M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                            | 0.00/1.50G [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'tokenizer_config.json' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/417d038a63fa3de29cfde265caedae14d1a58d92.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_config.json: 9.73kB [00:00, 60.0MB/s]A\u001b[A\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/417d038a63fa3de29cfde265caedae14d1a58d92\n",
      "Downloading '.gitattributes' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/52373fe24473b1aa44333d318f578ae6bf04b49b.incomplete'\n",
      "\n",
      "\n",
      "\n",
      ".gitattributes: 1.57kB [00:00, 16.5MB/s]A\u001b[A\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/52373fe24473b1aa44333d318f578ae6bf04b49b\n",
      "Fetching 10 files:  10%|██▌                      | 1/10 [00:00<00:04,  2.21it/s]Downloading 'vocab.json' to '/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/4783fe10ac3adce15ac8f358ef5462739852c569.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "vocab.json: 2.78MB [00:00, 131MB/s][A\u001b[A\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/4783fe10ac3adce15ac8f358ef5462739852c569\n",
      "\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 24.5MB/s]\u001b[A\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4\n",
      "\n",
      "\n",
      "model.safetensors:   0%|                   | 2.81M/1.50G [00:01<10:38, 2.35MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                   | 6.20M/1.50G [00:01<05:12, 4.79MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   1%|▏                  | 13.1M/1.50G [00:02<03:16, 7.60MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   2%|▍                  | 31.6M/1.50G [00:02<01:02, 23.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   3%|▌                  | 43.4M/1.50G [00:02<01:04, 22.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5%|▊                  | 68.4M/1.50G [00:02<00:32, 44.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|█                  | 84.5M/1.50G [00:03<00:29, 48.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   7%|█▍                  | 111M/1.50G [00:03<00:22, 63.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   8%|█▋                  | 123M/1.50G [00:03<00:23, 58.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|█▊                  | 141M/1.50G [00:03<00:18, 72.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  10%|██                  | 153M/1.50G [00:03<00:19, 69.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|██▏                 | 165M/1.50G [00:04<00:20, 66.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  12%|██▎                 | 174M/1.50G [00:04<00:19, 68.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  12%|██▍                 | 187M/1.50G [00:04<00:17, 76.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  13%|██▋                 | 200M/1.50G [00:04<00:16, 77.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  15%|██▉                 | 225M/1.50G [00:04<00:15, 82.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  16%|███▏                | 237M/1.50G [00:05<00:16, 77.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|███▎                | 250M/1.50G [00:05<00:15, 78.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  18%|███▌                | 264M/1.50G [00:05<00:21, 58.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  19%|███▊                | 288M/1.50G [00:05<00:18, 66.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|████                | 307M/1.50G [00:06<00:16, 72.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|████▍               | 332M/1.50G [00:06<00:15, 75.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  24%|████▋               | 357M/1.50G [00:06<00:16, 69.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  26%|█████▌               | 395M/1.50G [00:06<00:10, 101MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  27%|█████▊               | 413M/1.50G [00:07<00:10, 107MB/s]\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model.safetensors:  30%|██████              | 458M/1.50G [00:07<00:11, 88.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  33%|███████              | 503M/1.50G [00:07<00:08, 115MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  35%|███████▎             | 522M/1.50G [00:08<00:08, 113MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  36%|███████▏            | 536M/1.50G [00:08<00:09, 98.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  37%|███████▋             | 555M/1.50G [00:08<00:09, 102MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  38%|███████▉             | 572M/1.50G [00:08<00:08, 107MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  39%|███████▊            | 584M/1.50G [00:08<00:10, 88.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  41%|████████▌            | 611M/1.50G [00:09<00:08, 106MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  42%|████████▊            | 630M/1.50G [00:09<00:08, 101MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  44%|████████▋           | 655M/1.50G [00:09<00:08, 99.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  45%|█████████           | 680M/1.50G [00:09<00:09, 87.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  46%|█████████▊           | 698M/1.50G [00:09<00:08, 100MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  48%|█████████▌          | 717M/1.50G [00:10<00:09, 79.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  57%|████████████         | 863M/1.50G [00:10<00:04, 156MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  59%|████████████▍        | 888M/1.50G [00:11<00:03, 156MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  63%|█████████████▏       | 942M/1.50G [00:11<00:02, 194MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  68%|█████████████▋      | 1.03G/1.50G [00:11<00:01, 252MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  76%|███████████████▏    | 1.14G/1.50G [00:11<00:01, 346MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  86%|█████████████████▏  | 1.29G/1.50G [00:12<00:00, 370MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  96%|███████████████████ | 1.44G/1.50G [00:12<00:00, 338MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|████████████████████| 1.50G/1.50G [00:12<00:00, 118MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/blobs/f47f71177f32bcd101b7573ec9171e6a57f4f4d31148d38e382306f42996874b\n",
      "Fetching 10 files: 100%|████████████████████████| 10/10 [00:13<00:00,  1.32s/it]\n",
      "/home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca\n"
     ]
    }
   ],
   "source": [
    "!hf download Qwen/Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Configure environment variables for optimal Neuron performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Neuron compiler and runtime settings\n",
    "os.environ['NEURON_CC_FLAGS'] = \"--model-type transformer --retry_failed_compilation\"\n",
    "os.environ['NEURON_FUSE_SOFTMAX'] = \"1\"\n",
    "os.environ['NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS'] = \"3\"\n",
    "os.environ['MALLOC_ARENA_MAX'] = \"64\"\n",
    "os.environ['WORLD_SIZE'] = \"8\"\n",
    "os.environ['WANDB_DISABLED'] = \"true\"  # Disable wandb logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Define hyperparameters for the distillation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-0.6B\n",
      "Dataset: data/chess_output.json\n",
      "Output directory: Qwen3-0.6B-chess-finetuned\n",
      "Temperature: 4.0, Alpha: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "PROCESSES_PER_NODE = 2  # Distributed training processes\n",
    "NUM_EPOCHS = 3  # Number of training epochs\n",
    "TP_DEGREE = 2  # Tensor parallelism degree\n",
    "BS = 1  # Batch size per device\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Effective batch size = 16\n",
    "LOGGING_STEPS = 1  # Log every step\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "OUTPUT_DIR = \"Qwen3-0.6B-chess-finetuned\"\n",
    "DATASET_PATH = \"data/chess_output.json\"\n",
    "\n",
    "# Distillation hyperparameters\n",
    "TEMPERATURE = 4.0  # Softness of probability distributions\n",
    "ALPHA = 0.7  # Weight for soft loss (0.7 = 70% teacher, 30% labels)\n",
    "\n",
    "# Set max steps (use -1 for full training)\n",
    "MAX_STEPS = -1  # Train for full epochs\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Temperature: {TEMPERATURE}, Alpha: {ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Chess Dataset\n",
    "\n",
    "Check that the chess logits data from Lab 0 is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 100 valid chess samples\n",
      "✓ Average logit positions: 3.0\n",
      "\n",
      "Example:\n",
      "  Input: The FEN of the given chess board is \"1r4k1/4nppp/8/4Pb2/8/1P5P/r1PR4/3R3K w - - 0 27\". Which move is...\n",
      "  Expected: MoveA:d2d8\n",
      "  Generated: system\n",
      "Classify the better move. Output format: MoveA or MoveB\n",
      "user\n",
      "The FEN of the given chess board is \"1r4k1/4nppp/8/4Pb2/8/1P5P/r1PR4/3R3K w - - 0 27\". Which move is better? MoveA:d2d8, Adjust the piece to a key area, where it holds more influence over the board. TacticA: d2d8 b8d8 d1d8 Checkmate!  MoveB:d2d7, Switch the piece to a more advantageous place, increasing its mastery over the board. TacticB: d2d7 f5d7 Trade the lower value piece for a higher value piece. \n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "MoveA\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(DATASET_PATH).exists():\n",
    "    print(f\"ERROR: {DATASET_PATH} not found!\")\n",
    "    print(\"Please run Lab0_generate_teacher_logits_chess.ipynb first.\")\n",
    "else:\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        chess_data = json.load(f)\n",
    "    \n",
    "    valid_samples = [s for s in chess_data if 'error' not in s]\n",
    "    print(f\"✓ Found {len(valid_samples)} valid chess samples\")\n",
    "    print(f\"✓ Average logit positions: {sum(len(s['response']['token_logits']) for s in valid_samples) / len(valid_samples):.1f}\")\n",
    "    \n",
    "    # Show example\n",
    "    sample = valid_samples[0]\n",
    "    print(f\"\\nExample:\")\n",
    "    print(f\"  Input: {sample['input'][:100]}...\")\n",
    "    print(f\"  Expected: {sample['expected_output']}\")\n",
    "    print(f\"  Generated: {sample['response']['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Execute the distributed training using `torchrun`.\n",
    "\n",
    "**Note**: First run will compile the model (~20-30 minutes). Subsequent runs use cached compilation.\n",
    "\n",
    "**Training Process:**\n",
    "1. **Compilation** (first run only): Neuron compiler optimizes model for Trainium\n",
    "2. **Training**: Student learns from teacher logits\n",
    "3. **Checkpointing**: Model saved to OUTPUT_DIR\n",
    "\n",
    "**Expected Time:**\n",
    "- Compilation: ~20-30 minutes (one-time)\n",
    "- Training (100 samples, 3 epochs): ~10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "This will take ~30-45 minutes on first run (includes compilation)\n",
      "\n",
      "Command:\n",
      "\n",
      "torchrun  \\\n",
      "    --nproc_per_node 2 \\\n",
      "    src/distill_chess_neuron_torchrun.py \\\n",
      "    --model_id Qwen/Qwen3-0.6B \\\n",
      "    --dataset_path data/chess_output.json \\\n",
      "    --output_model_path ./final_chess_model \\\n",
      "    --temperature 4.0 \\\n",
      "    --alpha 0.7 \\\n",
      "    --num_train_epochs 3 \\\n",
      "    --do_train \\\n",
      "    --max_steps -1 \\\n",
      "    --per_device_train_batch_size 1 \\\n",
      "    --gradient_accumulation_steps 16 \\\n",
      "    --learning_rate 1e-4 \\\n",
      "    --bf16 \\\n",
      "    --zero_1 False \\\n",
      "    --tensor_parallel_size 2 \\\n",
      "    --warmup_steps 5 \\\n",
      "    --pipeline_parallel_size 1 \\\n",
      "    --logging_steps 1 \\\n",
      "    --output_dir Qwen3-0.6B-chess-finetuned \\\n",
      "    --overwrite_output_dir\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1107 03:29:59.097000 56674 torch/distributed/run.py:774] \n",
      "W1107 03:29:59.097000 56674 torch/distributed/run.py:774] *****************************************\n",
      "W1107 03:29:59.097000 56674 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1107 03:29:59.097000 56674 torch/distributed/run.py:774] *****************************************\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:70: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private_kernels.blockwise_matmul_while'\n",
      "  warnings.warn(f\"Warning: {error}\")\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:70: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private_kernels.blockwise_matmul_while'\n",
      "  warnings.warn(f\"Warning: {error}\")\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "2025-11-07 03:30:13.879507: W neuron/pjrt-api/neuronpjrt.cc:1972] Use PJRT C-API 0.73 as client did not specify a PJRT C-API version\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "2025-11-07 03:30:13.904911: W neuron/pjrt-api/neuronpjrt.cc:1972] Use PJRT C-API 0.73 as client did not specify a PJRT C-API version\n",
      "2025-Nov-07 03:30:17.0057 56694:56808 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):219 CCOM WARN NET/OFI Failed to initialize rdma protocol\n",
      "2025-Nov-07 03:30:17.0061 56694:56808 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Nov-07 03:30:17.0066 56694:56808 [1] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Nov-07 03:30:17.0069 56694:56808 [1] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "2025-Nov-07 03:30:22.0261 56693:56851 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):219 CCOM WARN NET/OFI Failed to initialize rdma protocol\n",
      "2025-Nov-07 03:30:22.0264 56693:56851 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Nov-07 03:30:22.0268 56693:56851 [0] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Nov-07 03:30:22.0271 56693:56851 [0] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "[2025-11-07 03:30:22.362: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 2\n",
      "[2025-11-07 03:30:22.364: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-11-07 03:30:22.364: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-11-07 03:30:22.364: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-11-07 03:30:22.364: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 2\n",
      "2025-11-07 03:30:22.000389:  56693  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_4216092561315976987+bad9cf09/model.neff\n",
      "2025-11-07 03:30:22.000389:  56694  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_4216092561315976987+bad9cf09/model.neff\n",
      "PyTorch: setting up devices\n",
      "[2025-11-07 03:30:22.606: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x797875ea9a20>, 'Ascending Ring PG Group')>\n",
      "[2025-11-07 03:30:22.607: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-11-07 03:30:22.608: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-11-07 03:30:22.608: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-11-07 03:30:22.608: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-11-07 03:30:22.609: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-11-07 03:30:22.609: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "PyTorch: setting up devices\n",
      "Loading chess dataset from data/chess_output.json\n",
      "Loaded 100 valid chess samples\n",
      "Loading model: Qwen/Qwen3-0.6B\n",
      "Loading chess dataset from data/chess_output.json\n",
      "Loaded 100 valid chess samples\n",
      "Loading model: Qwen/Qwen3-0.6B\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:33: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:33: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors\n",
      "loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors\n",
      "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-0.6B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-0.6B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "`even_batches` in `AcceleratorConfig` is not supported in NeuronTrainer and will be ignored. Make sure that your dataset size is divisible by the train batch size x gradient accumulation steps x data parallel size.\n",
      "`use_seedable_sampler` in `AcceleratorConfig` is not supported in NeuronTrainer and will be ignored.\n",
      "Starting training...\n",
      "Starting training...\n",
      "Pipeline parallelsim: forcing the dataloader to drop the last incomplete batch because it can cause failure if the last batch size is not divisible by the number of microbatches for the pipeline.\n",
      "2025-11-07 03:30:53.000419:  56694  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_11196433285822111861+bad9cf09/model.neff\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 3\n",
      "  Data Parallel Size: 1\n",
      "  Tensor Parallel Size: 2\n",
      "  Pipeline Parallel Size: 1\n",
      "  Instantaneous batch size per data parallel rank = 1\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Total optimization steps = 21\n",
      "  Num trainable parameters = 596,049,920\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]2025-11-07 03:30:54.000253:  56693  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_11196433285822111861+bad9cf09/model.neff\n",
      "The following columns in the Training set don't have a corresponding argument in `Qwen3ForCausalLM.forward` and have been ignored: teacher_logits. If teacher_logits are not expected by `Qwen3ForCausalLM.forward`,  you can safely ignore this message.\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:507: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_training/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:507: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "2025-11-07 03:33:10.000224:  56693  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_17244610172249175385+bad9cf09/model.neff\n",
      "2025-11-07 03:33:10.000513:  56694  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_3876370926838788815+bad9cf09/model.neff\n",
      "2025-11-07 03:33:29.000862:  56693  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_6877101459563117558+bad9cf09/model.neff\n",
      "2025-11-07 03:33:30.000550:  56694  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_8623340492324789537+bad9cf09/model.neff\n",
      "  5%|██                                         | 1/21 [02:51<57:08, 171.42s/it]2025-11-07 03:33:48.000833:  56693  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_1193724274029669226+bad9cf09/model.neff\n",
      "2025-11-07 03:33:50.000222:  56694  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_1193724274029669226+bad9cf09/model.neff\n",
      "2025-11-07 03:36:09.000914:  56693  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_14232261780857735320+bad9cf09/model.neff\n",
      "{'loss': 4.0096, 'learning_rate': 2e-05, 'grad_norm': 56.0, 'epoch': 0.16}      \n",
      "  5%|██                                         | 1/21 [05:16<57:08, 171.42s/it]2025-11-07 03:36:11.000127:  56694  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_14232261780857735320+bad9cf09/model.neff\n",
      " 10%|████                                       | 2/21 [05:20<50:02, 158.04s/it]2025-11-07 03:36:17.000110:  56693  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_2974321072334943081+bad9cf09/model.neff\n",
      "2025-11-07 03:36:18.000425:  56694  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.21.18209.0+043b1bf7/MODULE_2974321072334943081+bad9cf09/model.neff\n",
      "{'loss': 3.9845, 'learning_rate': 4e-05, 'grad_norm': 56.5, 'epoch': 0.32}      \n",
      "{'loss': 2.956, 'learning_rate': 6e-05, 'grad_norm': 24.125, 'epoch': 0.48}     \n",
      "{'loss': 2.2485, 'learning_rate': 8e-05, 'grad_norm': 23.25, 'epoch': 0.64}     \n",
      " 24%|██████████▏                                | 5/21 [12:31<39:04, 146.51s/it]"
     ]
    }
   ],
   "source": [
    "# Build the training command\n",
    "training_cmd = f\"\"\"\n",
    "torchrun  \\\\\n",
    "    --nproc_per_node {PROCESSES_PER_NODE} \\\\\n",
    "    src/distill_chess_neuron_torchrun.py \\\\\n",
    "    --model_id {MODEL_NAME} \\\\\n",
    "    --dataset_path {DATASET_PATH} \\\\\n",
    "    --output_model_path ./final_chess_model \\\\\n",
    "    --temperature {TEMPERATURE} \\\\\n",
    "    --alpha {ALPHA} \\\\\n",
    "    --num_train_epochs {NUM_EPOCHS} \\\\\n",
    "    --do_train \\\\\n",
    "    --max_steps {MAX_STEPS} \\\\\n",
    "    --per_device_train_batch_size {BS} \\\\\n",
    "    --gradient_accumulation_steps {GRADIENT_ACCUMULATION_STEPS} \\\\\n",
    "    --learning_rate 1e-4 \\\\\n",
    "    --bf16 \\\\\n",
    "    --zero_1 False \\\\\n",
    "    --tensor_parallel_size {TP_DEGREE} \\\\\n",
    "    --warmup_steps 5 \\\\\n",
    "    --pipeline_parallel_size 1 \\\\\n",
    "    --logging_steps {LOGGING_STEPS} \\\\\n",
    "    --output_dir {OUTPUT_DIR} \\\\\n",
    "    --overwrite_output_dir\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take ~30-45 minutes on first run (includes compilation)\")\n",
    "print(\"\\nCommand:\")\n",
    "print(training_cmd)\n",
    "\n",
    "# Run training\n",
    "!{training_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate the shards\n",
    "\n",
    "The distilled model is saved as part of the script as a sharded checkpoint, where each model parallel worker is resposible for saving its shard of the model weights. In order to use the model for inference, we need to consolidate the model shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:70: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private_kernels.blockwise_matmul_while'\n",
      "  warnings.warn(f\"Warning: {error}\")\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "Consolidating checkpoints from ./final_chess_model to the safetensors format...\n",
      "Consolidated checkpoint saved at ./final_chess_model\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli neuron consolidate ./final_chess_model ./final_chess_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "Check the training output and saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to ./final_chess_model\n",
      "\n",
      "Model files:\n",
      "total 1.2G\n",
      "-rw-r--r-- 1 ubuntu ubuntu  707 Nov  6 22:58 added_tokens.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 4.1K Nov  6 22:58 chat_template.jinja\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.4K Nov  6 22:58 config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.6M Nov  6 22:58 merges.txt\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.2G Nov  7 01:01 model.safetensors\n",
      "drwxr-xr-x 4 ubuntu ubuntu 4.0K Nov  6 22:58 shards\n",
      "-rw-r--r-- 1 ubuntu ubuntu  613 Nov  6 22:58 special_tokens_map.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu  11M Nov  6 22:58 tokenizer.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 5.3K Nov  6 22:58 tokenizer_config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 4.1K Nov  6 22:58 training_args.bin\n",
      "-rw-r--r-- 1 ubuntu ubuntu  515 Nov  6 22:58 trn_config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.7M Nov  6 22:58 vocab.json\n"
     ]
    }
   ],
   "source": [
    "# Check if model was saved\n",
    "final_model_path = \"./final_chess_model\"\n",
    "\n",
    "if Path(final_model_path).exists():\n",
    "    print(f\"✓ Model saved to {final_model_path}\")\n",
    "    print(f\"\\nModel files:\")\n",
    "    !ls -lh {final_model_path}\n",
    "else:\n",
    "    print(f\"✗ Model not found at {final_model_path}\")\n",
    "    print(\"Training may have failed. Check the output above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully:\n",
    "- ✓ Loaded chess move evaluation dataset with teacher logits\n",
    "- ✓ Configured knowledge distillation training\n",
    "- ✓ Trained a 0.6B student model from a 30B teacher\n",
    "- ✓ Saved the trained model for inference\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to Lab 2 to test the trained model\n",
    "- Compare student vs teacher predictions\n",
    "- Measure inference speed improvements\n",
    "\n",
    "**Model Compression:**\n",
    "- Teacher: 30B parameters\n",
    "- Student: 0.6B parameters\n",
    "- **Reduction**: 50x smaller!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
