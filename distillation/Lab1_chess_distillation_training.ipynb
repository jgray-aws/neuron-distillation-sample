{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Chess Move Evaluation - Knowledge Distillation Training\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will train a smaller \"student\" model to evaluate chess moves using knowledge distillation. This builds on Lab 0, where you generated teacher model logits from the Qwen3-30B-A3B model.\n",
    "\n",
    "**Task**: Train a student model to classify which chess move is better (MoveA or MoveB)\n",
    "\n",
    "**Why Knowledge Distillation for Chess?**\n",
    "- **Cost Reduction**: 50x smaller model (30B → 0.6B parameters)\n",
    "- **Faster Inference**: ~20-50x faster move evaluation\n",
    "- **Deployment Flexibility**: Can run on smaller instances or edge devices\n",
    "- **Maintained Performance**: Retains much of the teacher's chess understanding\n",
    "\n",
    "**Training Approach:**\n",
    "\n",
    "The `KnowledgeDistillationTrainer` combines two loss functions:\n",
    "1. **Hard Loss**: Cross-entropy with true labels (MoveA or MoveB)\n",
    "2. **Soft Loss**: KL divergence between teacher and student logits\n",
    "\n",
    "Combined loss: `total_loss = α × soft_loss + (1 - α) × hard_loss`\n",
    "\n",
    "Where α=0.7 means 70% weight on learning from teacher, 30% on correct answers.\n",
    "\n",
    "**Models:**\n",
    "- **Teacher**: Qwen3-30B-A3B (30 billion parameters)\n",
    "- **Student**: Qwen3-0.6B (600 million parameters)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Lab 0 with chess logits saved to `data/chess_output.json`\n",
    "- AWS Trainium instance (trn1.32xlarge recommended)\n",
    "- AWS Neuron SDK installed\n",
    "- Virtual environment: `/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Student Model\n",
    "\n",
    "Download the Qwen3-0.6B model weights from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download Qwen/Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Configure environment variables for optimal Neuron performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Neuron compiler and runtime settings\n",
    "os.environ['NEURON_CC_FLAGS'] = \"--model-type transformer --retry_failed_compilation\"\n",
    "os.environ['NEURON_FUSE_SOFTMAX'] = \"1\"\n",
    "os.environ['NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS'] = \"3\"\n",
    "os.environ['MALLOC_ARENA_MAX'] = \"64\"\n",
    "os.environ['WORLD_SIZE'] = \"8\"\n",
    "os.environ['WANDB_DISABLED'] = \"true\"  # Disable wandb logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Define hyperparameters for the distillation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-0.6B\n",
      "Dataset: data/chess_output.json\n",
      "Output directory: Qwen3-0.6B-chess-finetuned\n",
      "Temperature: 4.0, Alpha: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "PROCESSES_PER_NODE = 2  # Distributed training processes\n",
    "NUM_EPOCHS = 3  # Number of training epochs\n",
    "TP_DEGREE = 2  # Tensor parallelism degree\n",
    "BS = 1  # Batch size per device\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Effective batch size = 16\n",
    "LOGGING_STEPS = 1  # Log every step\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "OUTPUT_DIR = \"Qwen3-0.6B-chess-finetuned\"\n",
    "DATASET_PATH = \"data/chess_output.json\"\n",
    "\n",
    "# Distillation hyperparameters\n",
    "TEMPERATURE = 4.0  # Softness of probability distributions\n",
    "ALPHA = 0.7  # Weight for soft loss (0.7 = 70% teacher, 30% labels)\n",
    "\n",
    "# Set max steps (use -1 for full training)\n",
    "MAX_STEPS = -1  # Train for full epochs\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Temperature: {TEMPERATURE}, Alpha: {ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Chess Dataset\n",
    "\n",
    "Check that the chess logits data from Lab 0 is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 100 valid chess samples\n",
      "✓ Average logit positions: 3.0\n",
      "\n",
      "Example:\n",
      "  Input: The FEN of the given chess board is \"1r4k1/4nppp/8/4Pb2/8/1P5P/r1PR4/3R3K w - - 0 27\". Which move is...\n",
      "  Expected: MoveA:d2d8\n",
      "  Generated: system\n",
      "Classify the better move. Output format: MoveA or MoveB\n",
      "user\n",
      "The FEN of the given chess board is \"1r4k1/4nppp/8/4Pb2/8/1P5P/r1PR4/3R3K w - - 0 27\". Which move is better? MoveA:d2d8, Adjust the piece to a key area, where it holds more influence over the board. TacticA: d2d8 b8d8 d1d8 Checkmate!  MoveB:d2d7, Switch the piece to a more advantageous place, increasing its mastery over the board. TacticB: d2d7 f5d7 Trade the lower value piece for a higher value piece. \n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "MoveA\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(DATASET_PATH).exists():\n",
    "    print(f\"ERROR: {DATASET_PATH} not found!\")\n",
    "    print(\"Please run Lab0_generate_teacher_logits_chess.ipynb first.\")\n",
    "else:\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        chess_data = json.load(f)\n",
    "    \n",
    "    valid_samples = [s for s in chess_data if 'error' not in s]\n",
    "    print(f\"✓ Found {len(valid_samples)} valid chess samples\")\n",
    "    print(f\"✓ Average logit positions: {sum(len(s['response']['token_logits']) for s in valid_samples) / len(valid_samples):.1f}\")\n",
    "    \n",
    "    # Show example\n",
    "    sample = valid_samples[0]\n",
    "    print(f\"\\nExample:\")\n",
    "    print(f\"  Input: {sample['input'][:100]}...\")\n",
    "    print(f\"  Expected: {sample['expected_output']}\")\n",
    "    print(f\"  Generated: {sample['response']['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Execute the distributed training using `torchrun`.\n",
    "\n",
    "**Note**: First run will compile the model (~20-30 minutes). Subsequent runs use cached compilation.\n",
    "\n",
    "**Training Process:**\n",
    "1. **Compilation** (first run only): Neuron compiler optimizes model for Trainium\n",
    "2. **Training**: Student learns from teacher logits\n",
    "3. **Checkpointing**: Model saved to OUTPUT_DIR\n",
    "\n",
    "**Expected Time:**\n",
    "- Compilation: ~20-30 minutes (one-time)\n",
    "- Training (100 samples, 3 epochs): ~10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "This will take ~30-45 minutes on first run (includes compilation)\n",
      "\n",
      "Command:\n",
      "\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/bin/torchrun  \\\n",
      "    --nproc_per_node 2 \\\n",
      "    src/distill_chess_neuron_torchrun.py \\\n",
      "    --model_id Qwen/Qwen3-0.6B \\\n",
      "    --dataset_path data/chess_output.json \\\n",
      "    --output_model_path ./final_chess_model \\\n",
      "    --temperature 4.0 \\\n",
      "    --alpha 0.7 \\\n",
      "    --num_train_epochs 3 \\\n",
      "    --do_train \\\n",
      "    --max_steps -1 \\\n",
      "    --per_device_train_batch_size 1 \\\n",
      "    --gradient_accumulation_steps 16 \\\n",
      "    --learning_rate 1e-4 \\\n",
      "    --bf16 \\\n",
      "    --zero_1 False \\\n",
      "    --tensor_parallel_size 2 \\\n",
      "    --warmup_steps 5 \\\n",
      "    --pipeline_parallel_size 1 \\\n",
      "    --logging_steps 1 \\\n",
      "    --output_dir Qwen3-0.6B-chess-finetuned \\\n",
      "    --overwrite_output_dir\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1105 14:59:30.564000 862950 torch/distributed/run.py:774] \n",
      "W1105 14:59:30.564000 862950 torch/distributed/run.py:774] *****************************************\n",
      "W1105 14:59:30.564000 862950 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1105 14:59:30.564000 862950 torch/distributed/run.py:774] *****************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/RIVChess/neuron-distillation-sample/distillation/src/distill_chess_neuron_torchrun.py\", line 31, in <module>\n",
      "    from transformers import AutoTokenizer, HfArgumentParser, DataCollatorForLanguageModeling\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2322, in _get_module\n",
      "    raise e\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 40, in <module>\n",
      "    from .auto_factory import _LazyAutoMapping\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 43, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2322, in _get_module\n",
      "    raise e\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/generation/utils.py\", line 128, in <module>\n",
      "    from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/accelerator.py\", line 36, in <module>\n",
      "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/checkpointing.py\", line 22, in <module>\n",
      "    from .utils import (\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/utils/__init__.py\", line 14, in <module>\n",
      "    from .ao import convert_model_to_fp8_ao, filter_first_and_last_linear_layers, has_ao_layers\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/utils/ao.py\", line 24, in <module>\n",
      "    from .imports import is_torchao_available, torchao_required\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/utils/imports.py\", line 35, in <module>\n",
      "    import torch_xla.core.xla_model as xm  # noqa: F401\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_xla/__init__.py\", line 211, in <module>\n",
      "    _found_libneuronxla = _aws_ec2_inf_trn_init()\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_xla/__init__.py\", line 152, in _aws_ec2_inf_trn_init\n",
      "    xla.init()\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla.py\", line 94, in init\n",
      "    initialize()\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_neuronx/initialization/__init__.py\", line 13, in initialize\n",
      "    _initializer = Initializer()\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_neuronx/initialization/initializer.py\", line 88, in __init__\n",
      "    subprocess.run([\"libneuronpjrt-path\"], stdout=subprocess.PIPE)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'libneuronpjrt-path'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/RIVChess/neuron-distillation-sample/distillation/src/distill_chess_neuron_torchrun.py\", line 31, in <module>\n",
      "    from transformers import AutoTokenizer, HfArgumentParser, DataCollatorForLanguageModeling\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2322, in _get_module\n",
      "    raise e\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 40, in <module>\n",
      "    from .auto_factory import _LazyAutoMapping\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 43, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2322, in _get_module\n",
      "    raise e\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/transformers/generation/utils.py\", line 128, in <module>\n",
      "    from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/accelerator.py\", line 36, in <module>\n",
      "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/checkpointing.py\", line 22, in <module>\n",
      "    from .utils import (\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/utils/__init__.py\", line 14, in <module>\n",
      "    from .ao import convert_model_to_fp8_ao, filter_first_and_last_linear_layers, has_ao_layers\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/utils/ao.py\", line 24, in <module>\n",
      "    from .imports import is_torchao_available, torchao_required\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/accelerate/utils/imports.py\", line 35, in <module>\n",
      "    import torch_xla.core.xla_model as xm  # noqa: F401\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_xla/__init__.py\", line 211, in <module>\n",
      "    _found_libneuronxla = _aws_ec2_inf_trn_init()\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_xla/__init__.py\", line 152, in _aws_ec2_inf_trn_init\n",
      "    xla.init()\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla.py\", line 94, in init\n",
      "    initialize()\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_neuronx/initialization/__init__.py\", line 13, in initialize\n",
      "    _initializer = Initializer()\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_neuronx/initialization/initializer.py\", line 88, in __init__\n",
      "    subprocess.run([\"libneuronpjrt-path\"], stdout=subprocess.PIPE)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'libneuronpjrt-path'\n",
      "W1105 14:59:35.586000 862950 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 863022 closing signal SIGTERM\n",
      "E1105 14:59:35.618000 862950 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 863021) of binary: /opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/bin/torchrun\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
      "    run(args)\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "src/distill_chess_neuron_torchrun.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-05_14:59:35\n",
      "  host      : ip-172-31-13-142.us-west-2.compute.internal\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 863021)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Build the training command\n",
    "training_cmd = f\"\"\"\n",
    "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/bin/torchrun  \\\\\n",
    "    --nproc_per_node {PROCESSES_PER_NODE} \\\\\n",
    "    src/distill_chess_neuron_torchrun.py \\\\\n",
    "    --model_id {MODEL_NAME} \\\\\n",
    "    --dataset_path {DATASET_PATH} \\\\\n",
    "    --output_model_path ./final_chess_model \\\\\n",
    "    --temperature {TEMPERATURE} \\\\\n",
    "    --alpha {ALPHA} \\\\\n",
    "    --num_train_epochs {NUM_EPOCHS} \\\\\n",
    "    --do_train \\\\\n",
    "    --max_steps {MAX_STEPS} \\\\\n",
    "    --per_device_train_batch_size {BS} \\\\\n",
    "    --gradient_accumulation_steps {GRADIENT_ACCUMULATION_STEPS} \\\\\n",
    "    --learning_rate 1e-4 \\\\\n",
    "    --bf16 \\\\\n",
    "    --zero_1 False \\\\\n",
    "    --tensor_parallel_size {TP_DEGREE} \\\\\n",
    "    --warmup_steps 5 \\\\\n",
    "    --pipeline_parallel_size 1 \\\\\n",
    "    --logging_steps {LOGGING_STEPS} \\\\\n",
    "    --output_dir {OUTPUT_DIR} \\\\\n",
    "    --overwrite_output_dir\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take ~30-45 minutes on first run (includes compilation)\")\n",
    "print(\"\\nCommand:\")\n",
    "print(training_cmd)\n",
    "\n",
    "# Run training\n",
    "!{training_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "Check the training output and saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model was saved\n",
    "final_model_path = \"./final_chess_model\"\n",
    "\n",
    "if Path(final_model_path).exists():\n",
    "    print(f\"✓ Model saved to {final_model_path}\")\n",
    "    print(f\"\\nModel files:\")\n",
    "    !ls -lh {final_model_path}\n",
    "else:\n",
    "    print(f\"✗ Model not found at {final_model_path}\")\n",
    "    print(\"Training may have failed. Check the output above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully:\n",
    "- ✓ Loaded chess move evaluation dataset with teacher logits\n",
    "- ✓ Configured knowledge distillation training\n",
    "- ✓ Trained a 0.6B student model from a 30B teacher\n",
    "- ✓ Saved the trained model for inference\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to Lab 2 to test the trained model\n",
    "- Compare student vs teacher predictions\n",
    "- Measure inference speed improvements\n",
    "\n",
    "**Model Compression:**\n",
    "- Teacher: 30B parameters\n",
    "- Student: 0.6B parameters\n",
    "- **Reduction**: 50x smaller!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
